% Chapter 4 - Topology Awareness

\glsresetall % reset the glossary to expand acronyms again
\chapter[Topology]{Topology Awareness}\label{ch:TopologyAwareness}
\index{Topology Awareness}

% Topology Awareness

\begin{itemize}
    \item Horovod, even though there's barely any improvement?
\end{itemize}

The first technique investigated to improve MPI collective communication performance is topology awareness.
The overarching idea is to accelerate computation by leveraging the knowledge of the underlying hardware.
Topology awareness is an often-used technique applied to many areas in both MPI and the greater HPC ecosystem.

This chapter builds on work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}, where the authors propose a method for applying topology awareness to allgather and broadcast.
Their work relies on the notion that collective algorithms have an implicit communication pattern and that the ranks in a communicator can be reordered to better fit the communication pattern to the host topology.
We start by extending their work to multiple new algorithms in allreduce and broadcast.
Microbenchmark evaluation shows that we can see up to 80\% performance improvement under certain scenarios, and our proposed mapping heuristics can outperform the established SCOTCH graph partitioning library \cite{Pellegrini2012SCOTCH}.

This chapter will start by outlining existing methods for applying topology awareness within MPI.
Next, we propose a method of accelerating MPI collectives and build an implementation targeting multiple MPI\_Allreduce and MPI\_Bcast algorithms.
Our work is evaluated in both CPU and GPU environments and on a fat-tree cluster.

\section{Motivation}
MPI provides a programming model so that processes on a distributed memory system can share data and work together.
MPI users are meant to assume that messages exchanged between any two ranks will have the same communication characteristics no matter what their ranks are. 
This abstraction provides a convenient programming environment allowing users to focus on building their application without worrying about the underlying hardware. 
Its simplicity also has the benefit of making portability easier, dissuading users from tying their application to a specific architecture.
But in reality, this assumption is false because modern computers are anything but simple. 
Compute nodes have expansive memory hierarchies spanning multiple caches, NUMA domains, and even GPU memory.
Furthermore, modern fabric interconnects have complex network topologies with variable performance depending on node location.
So, in reality, the hardware topology of the system has an outsized impact on message performance, which has a knock-on effect on overall application performance. 

MPI applications often have consistent and predictable data-transfer patterns, this means any one rank will often send and receive the same set of messages to and from the same set of peers on consecutive runs of the application.
Since data-transfer patterns are predictable, and the programming model doesn't bind processes to specific locations, this gives MPI implementations the flexibility to map ranks to processing elements to make the most optimal use of the underlying hardware.
This problem of mapping ranks to processing elements can be formalized as an instance of a graph-embedding problem. However, previous work has shown this to be an NP-Hard problem \cite{Hoefler2011GenericTopoMappingStrats}. 
Solving this problem at scale (on the order of millions of processes) is not feasible. Therefore heuristics and simplifications are often used instead.

To apply topology awareness to collectives, we can leverage how collective algorithms are structured as a series of point-to-point communications.
MPI providers will often include multiple algorithms for different collectives with common patterns, including ring, recursive doubling, knomial-tree, etc...
This gives MPI implementations the flexibility to select a performant algorithm depending on the collective's parameters like message size and the number of processes.
This framework for algorithm selection neglects the host topology, but there is room to incorporate it.
The collective algorithm defines the pattern to disseminate data, but the value of the ranks in the graph often does not matter (there are some algorithms where this does matter, like allgather, alltoall and commutative allreduce, but these can be accounted for).
So in this work, we evaluate the efficacy of mapping collective communication graphs to the underlying hardware topology on-the-fly during application runtime.

\section{Related works}
Topology awareness is a commonly used technique for accelerating MPI applications. 
Within MPI, there are many scenarios and methods where topology information can be applied to accelerate communication.
At the highest level, topology awareness can be applied to the application's entire communication graph \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016PTRAM, Faraji2016TopoAwareGPUSelection, Mirsadeghi2016MAGC, Galvez2017AutoTopoMap}.
These strategies require profiling the entire application to build the communication graph.
This communication graph is then used in future runs of the application to devise a mapping for each job allocation.
The efficiency of a mapping is evaluated using metrics such as hop-bytes or congestion.
So to achieve the best application performance, mapping algorithms try to minimize/maximize their chosen metric.
There are multiple research fronts with this strategy, efforts have gone into optimizing the metrics, the mapping algorithm itself, and the types of systems mappings are supported for.

Hoefler and Snir \cite{Hoefler2011GenericTopoMappingStrats} propose a general process mapping tool targeting CPU clusters.
They evaluate three algorithms, a greedy algorithm based on vertex weights, a recursive bisecting that makes minimum weighted edge-cuts, and a graph similarity mapping using the Reverse Cuthill McKee algorithm. 
They use these algorithms, along with a \textit{Threshold Accepting} optimization step, to minimize congestion and dilation on large-scale SMP clusters.

Mirsadeghi and Afsahi \cite{Mirsadeghi2016PTRAM} propose a system targeting large-scale Infiniband clusters.
Their system leverages the network topology plus Infiniband's static routing tables to further reduce congestion.
They propose a hybrid metric which is a linear combination of hop-bytes and three types of congestion statistics. 
The mapping, along with further refinements, are calculated using a parallel greedy algorithm.

Faraji et al. l \cite{Faraji2016TopoAwareGPUSelection} focus their efforts on building a system targeting intranode GPU communication.
While their system only works on a single node and relies on SCOTCH's \cite{Pellegrini2012SCOTCH} graph bisecting method to perform the mapping, Mirsadehi et al. \cite{Mirsadeghi2016MAGC} expand the work to a full cluster.
The complete system uses a 3-step process, first mapping ranks to nodes to minimize network communication, then ranks to core to optimize intranode communication, and lastly, the GPU-to-rank step for optimal GPU-to-GPU communications.

Existing systems struggle to manage collective communications.
The profiling stage is often built on top of the PMPI profiling interface, which can intercept MPI calls, but doesn't break collectives into their constituent point-to-point messages.
Galvez et al. \cite{Galvez2017AutoTopoMap} identify this problem and propose a profiler that groups types of communications into weighted classes.
Their system leverages a parallel algorithm that uses the communication classes, along with a set of weighted metrics, to calculate a near-ideal mapping.
While a step in the right direction, their solution still treats collective communications as a black box, focusing on the collective's communicator and not disassembling the collective into its constituent point-to-point messages.

In order to decompose collectives into point-to-point messages, Bosilica et al. \cite{Bosilica2017OnlineMonitoringMPI} implemented a monitoring layer in OpenMPI that is accessible through the MPI Tools interface.
This provides much more granular profiling for point-to-point messages, which allowed Jeannot and Sartori \cite{Jeannot2020ImprvMPICommMonitoring} to propose a rank reordering method for applications with iterative compute, but their solution is required to be implemented within the user's application. 
So while it is possible to apply topology awareness to all communication, including collectives, there is a large burden on application developers to implement it within their software.

One of the common shortcomings most of these systems have in common is the required application profiling.
The application needs to run at least once to build the communication graph, and this can be expensive for large-scale runs. 
An ideal solution would be able to build a graph for a moderate amount of processes and project a solution for a large-scale system, but no existing solution can perform this yet. 

The MPI standard does provide a few interfaces to allow application developers to inform the runtime of expected communication patterns.  
One of these solutions are virtual topologies. 
This is a structure attached to a communicator that tells the implementation how communications are likely to occur between ranks.
Mercer and Jeannot \cite{Mercer2011ImprvMPIWithRR} leverage the MPI\_Dist\_graph\_create endpoint to reorder processes on a graph topology.
Their solution centralizes the topology information and uses the TreeMatch algorithm \cite{Jeannot2010TreeMatch} to calculate a process reordering.
Gropp \cite{Gropp2019CartTopoMapping} proposes a method for reordering processes in a cartesian topology.
His method relies on MPI\_COMM\_TYPE\_SHARED and neglects network topology as well as memory hierarchy information.

The most common way to apply topology awareness within collectives is to use a hierarchical strategy.
This is done by splitting the communicator into intra-node and inter-node sub-communicators.
The intra-node communicator consists of processes that are all placed on the same node.
Each node selects a leader process which is a member of the inter-communicator.
The collective communication pattern is then implemented across the hierarchical structure, ensuring efficient hardware use between shared memory and network resources.

Lue et al. \cite{Luo2018ADAPT} propose a library for event-based collectives.
While the crux of their work is focused on minimizing synchronization dependencies within collectives, they do demonstrate how their work could easily be mapped to a hierarchical type algorithm.
Awan et al. \cite{Awan2016NCCLBcast} build a GPU-aware hierarchical bcast algorithm.
Their work leverages NCCL for optimal intra-node communication for large messages and relies on existing algorithms in MVAPICH2 for inter-node communication.

Subramoni et al. \cite{Subramoni2011SpeedAwareBcast} designed a broadcast algorithm that can take Infiniband topology and network speeds into account.
They gather network information using OFED management tools like ibnetdiscover and ibroute and use the information to build a reordered communicator using either a depth-first traversal or a breath-first traversal.
They only evaluate knomial and scatter-allgather collectives.

The work presented in this thesis is an extension of work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}.
Instead of building a communication graph to custom fit the host topology, this work leverages the existing algorithm but renumbers the ranks under the hood to better map the communication graph to the hardware.
Their work focuses on broadcast and allgather collectives, with remapping algorithms targeting ring, recursive doubling and binomial communication patterns.
We extend their work by looking at allreduce and the additional challenges the reduction operation adds to their method.
We also propose mapping algorithms for reduce-scatter-allgather, knomial and binary-tree communication patterns.

\section{Method}
When applying topology awareness to collectives there are two common strategies.
Either build a communication pattern that is designed to fit the hardware, or take an existing pattern and efficiently map it to the hardware.
This work uses the latter strategy, we take the collective algorithm selected by the implementation and silently renumber the processes to better utilze communication resources.
When a collective is called, the MPI library selects an algorithm based on the message size and number of processes.
The communication pattern is implicit to the collective algorithm, so we define a heuristic tailored to specific algorithms that calculate how ranks should be renumbered.
To make the calculation efficient and scalable, each proposed remapping heuristic leverages a greedy strategy, and an outline of the general strategy is given in algorithm \ref{alg:toporr-strat}.
The basic structure starts with a reference rank, all process remappings will try to use hardware as close to the reference rank as possible.
Then the heuristic loops through the ranks, selecting the next rank to map based on the communication pattern, updating the reference rank when required.

The mapping is enforced by creating a new communicator.
Each process calls MPI\_Comm\_split() requesting their new rank, and then the selected algorithm is run on the resulting communicator.
Since communicator creation is an expensive operation, this shadow communicator is cached so that it can be used again if the same communicator uses the same algorithm.

This structure is flexible and allows us to evaluate multiple types of collectives.
Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} propose heuristics targeting ring allgather, recursive doubling allgather, and binomial tree broadcast. 
We reimplement their ring and recursive doubling algorithms and evaluate them on allreduce collectives, and evaluate their binomial-broadcast heuristic in the context of a scatter-allgather broadcast.
To extend their work while targeting large message allreduce, we propose a new reordering heuristic for reduce-scatter-allgather allreduce.
Furthermore, since broadcast can be used as a component in allreudce, we also propose heuristics for two broadcast algorithms, knomial and binary tree broadcast.

% \lstset{style = bklstc}
% \lstset{label = lst:topo-generic-strategy}
% \lstset{caption = General greedy heuristic for topology-aware rank reordering.}
% \lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/TopoRR_strat.c}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/TopoRR_strat_pseudocode}

\subsection{Reduce-Scatter-Allgather Allreduce}
Reduce-scatter-allgather (RSA) allreduce, also known as Rabenseifner's algorithm, is a popular algorithm implemented in most MPI implementations and even used in external collective libraries like UCC \cite{UCC}.
The algorithm starts with each process performing a recursive-vector halving reduce-scatter, this distributes a segment of the fully reduced vector to each rank.
At this point, each rank needs to gather the other reduced segments from the other ranks, this is achieved through a recursive doubling allgather.
RSA is used for large message allreduce because it efficiently uses system bandwidth.
For an allreduce with $p$ processes on $n$ bytes of data, each rank sends $2((p-1)/p)n\beta$ bytes of data, linear scaling with message size is ideal for large message collectives.
In terms of structure, since both phases rely on a recursive pattern, data is only exchanged with ranks that are a power of 2 distance away.
The other important feature of RSA is that more data is exchanged with numerically closer ranks.
Figure \ref{fig:graph-rsa} demonstrates this phenomenon with eight processes.
Most data is exchanged with immediate neighbours, this accounts for the first stage of reduce-scatter and the last stage of the allgather.

So while designing our heuristic, we want to map communicating pairs that are a power of two distances away, focusing on small distances first.
With these goals in mind, we propose the heuristic in algorithm \ref{alg:rsa} to target the RSA algorithm.
This heuristics starts by mapping rank 0 (line 1), then walks to the power of 2 communication graph mapping processes that have not been mapped yet (lines 5-10). 
While looping through all the ranks, the reference rank is updated every two mappings (lines 11-14), this ensures that communicating pairs that are distance one away are mapped as close as possible. 

\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_graph_rsa}
% \lstset{label = lst:topo-rsa}
% \lstset{caption = Heuristic for rank reordering the reduce-scatter-allgather algorithm.}
% \lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_RSA.c}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/rsa_pseudocode}

\subsection{Binary Tree Broadcast}
Binary trees benefit from its simplicity.
Ranks receive one message and forward it to two child ranks.
The tree structure provides logarithmic scaling to the number of messages sent, so binary trees are often used for smaller message sizes. 
A typical structure for a binary tree is provided in Figure \ref{fig:graph-bin-tree}.
For a rank $r$, the height in the tree can be calculated as $h_r = \lfloor log_2(r+1) \rfloor$, and ranks will send messages to $r + 2^{h_r}$ and $r + 2^{h_r + 1}$
This leads to a structure where processes further down the tree are sending to ranks further and further away.
Furthermore, binary trees are constructed slightly lopsided,  as nodes are added to the left side of the tree before the right side. 
This can lead to situations where one direction has slightly more nodes than the other, this should be accounted for when designing the heuristic.

To ensure that ranks reside on the same node as their children, we propose a depth-first-traversal heuristic with the code provided in algorithm \ref{alg:bintree}.
We employ a recursive function to traverse the tree efficiently. 
We start by mapping the tree's root (line 1) and passing it in as the reference rank to the recursive function (line 2).
The recursive function starts by calculating the reference rank's height and child processes (lines 4-7). 
Then, depending on if the child processes are in the communicator, we bind them close to the reference rank and recuse with the new process set as the reference rank (lines 8-15).
To account for lopsidedness, the heuristics traverses the potentially smaller side first by mapping the numerically greater child first.
We found that this does a more consistent job mapping the tree to system architectures we evaluated on.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_graph_bintree}
% \lstset{label = lst:topo-bintree}
% \lstset{caption = Heuristic for rank reordering binary trees.}
% \lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_BinTree.txt}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/bin_tree_pseudocode}

\subsection{Knomial Broadcast}
Knomial is a generalization of a binomial tree, and its structure can be defined inductively.
For a knomial tree $T_{k,h}$ of height $h$ and radix $k$, if $h=0$ then $T_{k,h}$ is a single node. For $h>0$, $T_{k,h}$ is build from $k$ coppies of $T_{k,h-1}$, where the root of the first copy is the parent node of the other $k-1$ coppies.
Figure \ref{fig:graph-knomial} provides an example of a knomial tree with radix 4 and 16 processes.
While any value of $k$ can be used, implementations commonly use either 2 (binomial tree) or 4. 
While the tree's structure is heavily lopsided, in theory, this provides the opportunity to overlap message transfers at lower layers of the tree with transfers higher in higher layers.
Due to the inductive definitions, high $k$ value trees will have a lot of leaf nodes, as each subtree will contain of $k-1$ subtrees with $h=0$.

In algorithm \ref{alg:knomial}, we propose another depth-first traversal algorithm, focusing on mapping smaller subtrees first.
By focusing on subtrees we are trying to group bunches of leaf nodes on the same node as their parent.
Since this is a depth-first traversal, we use another recursive function to traverse the tree. 
Once again, we start by mapping the tree's root (line 1) and passing it in as the reference rank of the recursive function (line 2).
In the recursive function, we immediately map the $k-1$ subtrees of height 0 (lines 4-6).
Then we use a while loop to calculate the values for subtrees that are further away (lines 8-19).
If the root of a subtree does exist in the communicator, we map it as close as possible to the reference rank (line 14), calculate its depth (line 15), and recurse on it as the reference rank (line 16).

\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_graph_knomial}
% \lstset{label = lst:topo-knomial}
% \lstset{caption = Heuristic for rank reordering knomial trees.}
% \lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_Knomial.c}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/knomial_pseudocode}

\subsection{Hessam's work on Binomial, Ring, and Recursive Doubling}

Previous work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} proposed a series of algorithms targeting recursive doubling allgather, ring, binomial broadcast, and binomial gather. 
While their work is evaluated in the context of allgather, the reordering heuristics they propose can also be used to accelerate allreduce. 

The ring and recursive doubling allgather heuristics can be transplanted directly to ring and recursive doubling allreduce, respectively, and the intuition behind both algorithm's utilities generally holds. 
The ring targets nearest neighbour ranks, it starts at rank 0 and walks through the communicator mapping $r+1$ as close as possible to reference rank $r$, then setting $r+1$ as the new reference rank.
Since both ring allreduce and ring allgather only communicate with immediate neighbours (i.e. rank $r+1$ and $r-1$), the intuition of this heuristic is portable across collectives.
The recursive doubling algorithm targets the last two stages of the algorithm, as message sizes are the largest during those phases. 
It's similar to our proposed RSA heuristic but focuses on the last rounds of communication instead of the first. 
Recursive doubling allreduce and allgather share the same communication pattern, but while the message sizes double each round in allgather,  it remains fixed in allreduce.
So migrating the heuristic to allreduce might not see the same performance gains, but it will ensure that most communications take an efficient path.

We also evaluated the preveously proposed recursive doubling allgather heuristic in the context of the scatter-allgather broadcast.
Scatter-allgather, also known as Van de Geijn's algorithm, is often used for large message broadcasts as it is incredibly bandwidth efficient, 
This is achieved by splitting the message up into segments, scattering the segments across the communicator with a binomial algorithm, and then reconstructing the message with an allgather of the segments.
The recursive doubling allgather has already been studied in \cite{Mirsadeghi2016TopoAwareCollRR}, it has an even communication pattern where the last stages of the algorithm have the largest message sizes.
On the other hand, the binomial scatter is similar to the binomial broadcast algorithm, with the difference that the message size halves between stages. 
The communication graph generated by the binomial pattern is essentially a subgraph of recursive doubling, so it can be layered on top of recursive doubling without adding any new communication dependencies. 
Therefore, we applied the recursive doubling reordering heuristic to scatter allgather broadcasts.

\section{Experiments and Analysis}
To evaluate our new heuristics, as well as the previously proposed heuristics, we orchestrated everything into OpenMPI.
We compare our heuristics against SCOTCH, a commonly used graph partitioning library previously applied to topology-aware works.
Through microbenchmark evaluation, we are able to demonstrate that our method can improve allreduce performance, depending on the initial mapping.

\subsection{Software Implementation}
We implemented our method within OpenMPI v4.0.5 \cite{gabriel2004OpenMPI}.
Building on top of OpenMPI let us leverage the existing collective implementations as well as the algorithm selection mechanism.
OpenMPI also has support for Hwloc integrated within the runtime, making topology detection easier.

There are some caveats with collective rank reordering as not all collectives can support it seamlessly.
By default, MPI\_Allreduce algorithms assume operations are commutative, but a user can specify an operation to be non-commutative with respect to process's ranks. 
To support non-commutative operations, Allreduce algorithms must be carefully designed to ensure operations are applied in the correct order, but renumbering processes break that careful design.
Therefore, this work only supports commutative operations.
Furthermore, both broadcast heuristics assume rank 0 is the root which is not always true. 
So when rank 0 isn't the broadcast root, it needs to receive the data from the real root before it can start the broadcast on the shadow-communicator.
In retrospect, a smarter design would have been to assign virtual ranks by shifting each rank according to the root's value, this would make the root 0.
But I last looked at the code for remapping a year ago, and the performance difference provided by this step would be negligible, furthermore, OMB only calls bcast w/ root 0, and Horovod doesn't use bcast.

We leverage existing system topology detection tools to generate the host topology matrix.
Hwloc \cite{Broquedis2010hwloc} is a tool for gathering intranode topology information, this includes NUMA domains and cache hierarchies.
To extract the network topology, we started by collecting topology information using cluster-provided tools, ibnetdiscovee for Beluga's InfiniBand network and Opareport for Cedar's OmniPath fabric.
We then transformed both files into a common format and saved them to disk.
When creating a topology matrix, each node is responsible for generating the row corresponding to its rank, this is done by indexing to the topology file, and checking neighbouring ranks with Hwloc, then an allgather is performed to gather the final results.
This method is not nearly that scalable as the memory requirements are on the order of $O(n^2)$ with the number of processes.

While OpenMPI does support CUDA memory transfers, the implementation of CUDA-based allreduce copies the data to a temporary host buffer performs a CPU-based allreudce, then copies the data back up to the GPU.
While this does fulfill the MPI specification, it neglects to use the available GPU hardware like compute kernel reduction or GPU to GPU data transfers with NVLinks.
So we modified OpenMPI's RSA algorithm to make use of GPU hardware, this is also more in line with how modern collective libraries like NCCL and UCC implement allreduce \cite{UCC, NCCL}.

To compare our proposed heuristics against an established topology mapping tool, we also integrated the SCOTCH graph embedding tool \cite{Pellegrini2012SCOTCH} into the rank reordering method.
SCOTCH uses a graph bisecting method to perform graph partitioning and embedding, and it has been used in previous work to incorporate topology awareness \cite{Mirsadeghi2016TopoAwareCollRR}.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/init_mapping}
The rank reordering strategy finds performance improvements by rearranging the process to core bindings, which implies that any performance improvements are dependent on the initial process to core bindings.
MPI implementations provide flags for users to specify how processes should be distributed at job launch. 
While often not exhaustive, this interface does provide a starting point for users to optimize the process to node mappings.
Our method can calculate more complicated mappings, as well as mappings for subsets of processes for when applications use subcommunicators.  

In OpenMPI, users can specify a resource and processes are bound to that resource in a round-robin fashion.
Figure \ref{fig:init-mappings} provides an example outlining the three initial mappings we evaluated.
The default initial mapping is by-package, this takes a node and fills it with processes alternating between packages until the node is full, it then repeats this on the next node until all processes are bound.
By-core mapping places processes to consecutively numbered cores filling up a node before moving to the next, and by-node scatters processes across nodes by cycling through nodes and placing processes one at a time.

\subsection{Hardware}\label{sec:topo-eval-hardware}
% We evaluated our work on two heterogeneous clusters, Beluga and Cedar, provided by Compute Canada. 
% Beluga is Infiniband based, structured as a 5:1 blocking fat tree with static routing.  
% The CPU nodes have dual socket Xeon Gold 6148 Skylake processors, and the GPU nodes use the same processors attached to 4 V100s fully connected with NVLink.
% Cedar, on the other hand, is OmniPath based.
% It also has a fat-tree topology configured, bus it is configured with 2:1 blocking and has adaptive routing.
% Cedar's CPU allocation has dual-socket 24-core Xeon Platinum 8260 Cascade Lake processors, while the GPU nodes have two 20-core Xeon Silver 4216 Cascade Lake processors hosting four v100s GPUs interconnected by NVLink.

We evaluated our work on Beluga, a heterogeneous cluster hosted by Polytechnique Montréal and access provided by Compute Canada. 
Beluga is Infiniband based, structured as a 5:1 blocking fat tree with static routing.  
The CPU nodes have dual socket Xeon Gold 6148 Skylake processors, and the GPU nodes use the same processors attached to 4 V100s fully connected with NVLink.

\subsection{Results}

\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/ac_frac_imprv_beluga}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/bc_frac_imprv_beluga}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/dual_64_1024}

We used OSU-Microbenchamrks v5.7 \cite{Bureddy2012OMB} to evaluate the performance of our proposed method. 
This tool performs 1000 loops over MPI\_Allreduce, measures the runtime of each function call, and outputs the average latency. 
It also provides the option to locate the data buffer in either CPU memory or GPU memory.

Figure \ref{fig:beluga-ar-frac-imprv} demonstrates allreduce's rank reordering for both CPU and GPU nodes.
GPU jobs were evaluated with 16 nodes (64 GPUs total), while CPU jobs were run across 32 nodes with 32 processes per node (1024 ranks total).
The ring and RSA algorithms show ideal performance in that they're often on-par or better than the initial mapping, save for a few outliers.
This shows how the proposed heuristics are finding a near ideal mapping or at least a mapping equivalent to what's provided as stock.  

One immediately obvious observation is that any improvements are more pronounced on CPUs vs GPUs, but it can be explained by the amount of contention on underlying resources.
When using CPUs, any cross-socket communication needs to traverse the QPI link, which can easily become a bottleneck when multiple processes try to concurrently send large messages across it.
The best example for this would be a ring algorithm with a by-package initial mapping.
With a by-package mapping, consecutive ranks are placed on different sockets, and since communications in ring only happen between consecutive ranks, all intranode communication needs to traverse the QPI.
Reordering fixes this by rearranging consecutive ranks to be as close as possible, leading to only one message needing to traverse the QPI link, removing QPI contention and providing up to 50\% performance improvements. 
But GPUs don't have the same contention since there are only four processes per node, and the demand for resources is much smaller, further, the GPUs are fully connected with NVLINK intranode, so there is no single link bottleneck.

The next thing to note would be how the most dramatic performance improvements are seen on the by-node mapping, specifically for ring and RSA. 
The by-node initial mapping places the most intense communications of the RSA algorithm on inter-node links, and literally, all communication in the ring algorithm is forced over the network.
The reordering results in massive performance improvements by moving large chunks of communication internode and provides up to 80\% improvement for ring and RSA on CPUs.

In retrospect, using recursive doubling is dumb, it produces mappings where one process per node has the bulk of its partners on the same node, but the rest of the processes need to exchange their data off the node. 
It's hard to notice with small communicator sizes, but when you scale it up so that $numnodes > processes per node$, it becomes more evident.
Now, since I feel like a complete idiot for proposing this idea, I feel like I should scrap it and pretend it never happened?

The results for the evaluated broadcast remappings are presented in figure \ref{fig:beluga-bc-frac-imprv}.
The broadcast algorithms see substantial improvements across the board. 
Many of the same characteristics map to the broadcast remappings, CPU results are much more dramatic than GPU results as there is more resource contention, and improvements are also greater when communication can be pulled off the network. 
The most obvious example of saving network resources would be the by-node knomial broadcast, as this moves all child communications from off-rank to within the same node, where they can leverage more efficient shared memory communications. 
Another example of saving network resources is the binary tree broadcast.
The by-package and by-core mappings see between 50\% to 80\% performance improvements across both GPU and CPU allocations, but near 0 improvements for by-node, this outlines how OpenMPI's binary tree implementation drives most of the communication to the network for core/socket initial mappings. 

Scatter allgather is an interesting case, as it can see strong improvements for by-core and by-package initial mappings, showing how moving the large transfers in binomial scatter can save time. 
Interestingly though, it loses performance on a by-node mapping, so clearly, there is performance left on the table, and more work could be done.

In a lot of cases, the rank reordering generates a mapping similar to the initial mapping.
For example, the by-core initial mapping is quite similar to ring, RSA, and knomial, while the binary tree is most similar to the by-node.
In these scenarios, the performance is identical between the normal and remapped runs.

To compare our work against existing reordering heuristics, we integrated SCOTCH \cite{Pellegrini2012SCOTCH} into our method and show its performance results in figure \ref{fig:beluga-mandm-dual-64-1024}.
Once again, we ran with 16 GPU nodes and 32 CPU nodes, but this time we used OpenMPI's tuning table to select which algorithm to use based on its tuning table.
Each heuristic provides a more efficient mapping than anything SCOTCH can generate, as is evidenced by the superior performance. 
That said, OpenMPI's algorithm selection works efficiently for by-core and by-node mappings, but we are able to provide improvements when using a by-node mapping.  

\section{I can't believe it's not a conclusion ™}
High-level recap
Connect to chapter 5 

% Conseptualy, this problem can be treated as a graph mapping problem. 
% The MPI application is modeld as weighted graph $G=(V_G, w_G)$, where verticeis are processes and weights are the amount of communication between processes.
% The host topology is also models as a weighted graph $H=(V_h, w_H)$, where vertices are processing elements and weights represent the capacity of the interconnect between any two processing elements.
% % $G$ and $H$ are both fully connected graphs, and $|V_G| = |V_H|$.
% When running the application, the grpah $G$ is mapped to $H$ so that each rank in $V_G$ is overlayed on a processing element in $V_H$, and communication weights in $w_G$ are tied to a hardware link $w_H$.
% Lastly, a mathematical metric is defined to estimate the performance of the mapping.
% So the goal of topology-awareness is to find a mapping from $G$ to $H$ that minimized/maximized a defined metric.
% This problem, which is a subset of graph-embedding, is NP-hard, so finding optimal solutions for large scale instances of this problem is not feasable \cite{Hoefler2011GenericTopoMappingStrats}.
% Solutions used in practice often rely on heuristics to find near-optimal solutions in a reasonable amount of time.