% Chapter 4 - Topology Awareness

% \glsresetall % reset the glossary to expand acronyms again
\chapter[Topology Awaeness]{Topology Awareness}\label{ch:TopologyAwareness}
\index{Topology Awareness}

The first technique investigated to improve MPI collective communication performance is topology awareness.
The overarching idea is to accelerate communication by leveraging the knowledge of the underlying hardware.
Topology awareness is an often-used technique applied to many areas in both MPI and the greater HPC ecosystem.

This chapter builds on work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}, where the authors propose a method for applying topology awareness to allgather and broadcast.
Their work relies on the notion that collective algorithms have an implicit communication pattern and that the ranks in a communicator can be reordered to better fit the communication pattern to the host topology.
We extend their work to multiple new algorithms in allreduce and broadcast.
We see up to \_\_\% performance improvement under certain scenarios with microbenchmark evaluation, and our proposed mapping heuristics can outperform established techniques.
We also evaluated several deep learning applications, but the performance benefits present in the microbenchmarks did not translate.

This chapter will start by motivating the usage of topology awareness and outline existing methods using topology awareness within MPI.
Next, we propose a method of accelerating MPI collectives and build an implementation targeting multiple \texttt{MPI\_Allreduce()} and \texttt{MPI\_Bcast()} algorithms.
Our work is evaluated in CPU and GPU environments on two different clusters.

\section{Motivation}
MPI provides a programming model so processes on a distributed memory system can share data and work together.
MPI users are meant to assume that messages exchanged between any two ranks will have the same communication characteristics no matter what their ranks are. 
This abstraction provides a convenient programming environment allowing users to focus on building their application without worrying about the underlying hardware. 
Its simplicity also has the benefit of making portability easier, dissuading users from tying their application to a specific architecture.
But in reality, this assumption is false because modern computers are anything but simple. 
Compute nodes have expansive memory hierarchies spanning multiple caches, NUMA domains, and even GPU memory.
Furthermore, modern fabric interconnects have complex network topologies with variable performance depending on node location.
So, in reality, the hardware topology of the system has an outsized impact on message performance, which has a knock-on effect on overall application performance. 

Performance improvements with topology awareness invovles combining topology information with communication information, we have tools for collective topology information at runtime, and the collective's communication pattern is implicit to whichever algorithm they use.  
Different algorithms (ring, recursive doubling, binary tree, binomial tree, etc...) have predefined data-transfer patterns, we know ahead of time how ranks will send and receive messages, and we can trivially model these relationships for different sizes of communicators.
While there are existing methods for topology-aware collectives, they can add extra communication and complexity to support their algorithm. 
So this work evaluates and extends an existing method for adding topology awareness to flat algorithms by rank reordering.

\section{Related works}
Topology awareness is a commonly used technique for accelerating MPI applications. 
Within MPI, there are many scenarios and methods where topology information can be applied to accelerate communication.
At the highest level, topology awareness can be applied to the application's entire communication graph \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016PTRAM, Faraji2016TopoAwareGPUSelection, Mirsadeghi2016MAGC, Galvez2017AutoTopoMap}.
These strategies require profiling the entire application to build the communication graph.
This communication graph is then used in future runs of the application to devise a mapping for each job allocation.
The efficiency of a mapping is evaluated using metrics such as hop-bytes or congestion.
So to achieve the best application performance, mapping algorithms try to minimize/maximize their chosen metric.
There are multiple research fronts with this strategy, efforts have gone into optimizing the metrics, the mapping algorithm itself, and the types of systems mappings are supported for.

Hoefler and Snir \cite{Hoefler2011GenericTopoMappingStrats} propose a general process mapping tool targeting CPU clusters.
They evaluate three algorithms, a greedy algorithm based on vertex weights, a recursive bisecting that makes minimum weighted edge-cuts, and a graph similarity mapping using the Reverse Cuthill McKee algorithm. 
They use these algorithms, along with a \textit{Threshold Accepting} optimization step, to minimize congestion and dilation on large-scale SMP clusters.

Mirsadeghi and Afsahi \cite{Mirsadeghi2016PTRAM} propose a system targeting large-scale Infiniband clusters.
Their system leverages the network topology plus Infiniband's static routing tables to further reduce congestion.
They propose a hybrid metric which is a linear combination of hop-bytes and three types of congestion statistics. 
The mapping, along with a refinement step, are calculated using a parallelized greedy algorithm.

Faraji et al. l \cite{Faraji2016TopoAwareGPUSelection} focus their efforts on building a system targeting intranode GPU communication.
While their system only works on a single node and relies on SCOTCH's \cite{Pellegrini2012SCOTCH} graph bisecting method to perform the mapping, Mirsadehi et al. \cite{Mirsadeghi2016MAGC} expand the work to a full cluster.
The complete system uses a 3-step process, first mapping ranks to nodes to minimize network communication, then ranks to core to optimize intranode communication, and lastly, the GPU-to-rank step for optimal GPU-to-GPU communications.

These methods excel at optimizing for unique application communication patterns but also tend to neglect collective communications.
The profiling stage is often built on top of the PMPI profiling interface, which can intercept MPI calls, but doesn't break collectives into their constituent point-to-point messages.
Galvez et al. \cite{Galvez2017AutoTopoMap} identify this problem and propose a profiler that groups types of communications into weighted classes.
Their system leverages a parallel algorithm that uses the communication classes, along with a set of weighted metrics, to calculate a near-ideal mapping.
While a step in the right direction, their solution still treats collective communications as a black box, focusing on the collective's communicator and not disassembling the collective into its constituent point-to-point messages.

To decompose collectives into point-to-point messages, Bosilica et al. \cite{Bosilica2017OnlineMonitoringMPI} implemented a monitoring layer in OpenMPI that is accessible through the MPI Tools interface.
This provides much more granular profiling for point-to-point messages, which allowed Jeannot and Sartori \cite{Jeannot2020ImprvMPICommMonitoring} to propose a rank reordering method for applications with iterative compute, but their solution is required to be implemented within the user's application. 
So while it is possible to apply topology awareness to all communication, including collectives, their solution places a large burden on application developers to implement it within their software.

One of the common shortcomings most of these systems have in common is the required application profiling.
The application needs to run at least once to build the communication graph, and this can be expensive for large-scale runs. 
An ideal solution would be able to build a graph for a moderate amount of processes and project a solution for a large-scale system, but no existing solution can perform this yet. 

The MPI standard does provide a few interfaces to allow application developers to inform the runtime of expected communication patterns.  
One of these solutions is virtual topologies. 
This is a structure attached to a communicator that tells the implementation how communications are likely to occur between ranks.
Mercer and Jeannot \cite{Mercer2011ImprvMPIWithRR} leverage the \texttt{MPI\_Dist\_graph\_create()} endpoint to reorder processes on a graph topology.
Their solution centralizes the topology information and uses the TreeMatch algorithm \cite{Jeannot2010TreeMatch} to calculate a process reordering.
Gropp \cite{Gropp2019CartTopoMapping} proposes a method for reordering processes in a cartesian topology.
His method relies on \texttt{MPI\_COMM\_TYPE\_SHARED} and neglects network topology as well as memory hierarchy information.

The most common way to apply topology awareness within collectives is to use a hierarchical strategy.
This is done by splitting the main communicator into smaller sub-communicators, often one for inter-node resources and one for intra-node resources.
The intra-node communicator consists of ranks that are all placed on the same node, and each node selects a leader process and the leaders across all nodes for the inter-communicator.
The collective communication is then decomposed into stages of sub-collectives and  which are executed across the hierarchical structure.
For example, an allreduce could be implemented in 4 steps: first, an intra-node reduce to the leader, followed by a leader-wise internode reduce to some pre-determined rank, third a leader-wise internode broadcast is performed, and finally an intranode broadcast to finish the allreduce.
This method ensures efficient hardware use between shared memory and network resources by placing different stages on different resource layers.

Lue et al. \cite{Luo2018ADAPT} propose a library for event-based collectives.
While the crux of their work is focused on minimizing synchronization dependencies within collectives, they do demonstrate how their work could easily be mapped to a hierarchical algorithm.

Awan et al. \cite{Awan2016NCCLBcast} build a GPU-aware hierarchical broadcast algorithm.
Their work leverages NCCL for optimal intra-node communication for large messages and relies on existing algorithms in MVAPICH2 for inter-node communication.

Subramoni et al. \cite{Subramoni2011SpeedAwareBcast} designed a broadcast algorithm that can take Infiniband topology and network speeds into account.
They gather network information using OFED management tools like ibnetdiscover and ibroute and use the information to build a reordered communicator using either a depth-first traversal or a breath-first traversal.
They only evaluate knomial and scatter-allgather collectives.

Hierarchical algorithms segment the system topology and structure the collective to use appropriate resources accordingly, but the increased complexity has its drawbacks.
Decomposing the collective into stages introduces additional implicit barriers and synchronizations, hampering performance. 
While this can be alleviated through pipelining between stages, this adds further complexity and can lead to code maintenance issues.
Further, there are still design challenges with the granularity of sub-communicators.
Stages can be specified for hardware components like CPU cache layers, NUMA domains, or different layers of the network topology, allowing for more optimal hardware usage, but more stages add more memory requirements (for more sub-communicators), and more implicit synchronizations between stages.
Also, introducing multiple stages can create additional data transfers compared to a flat algorithm.
Take allreduce as an example, two reductions + two broadcasts can require more data transfers than a flat RSA or a ring allreduce, so the more naive algorithm could be faster in some scenarios. 
Our method mitigates many of these issues as it doesn't rely on multiple sub-communicators or algorithm stages to complete the operation.

The work presented in this thesis is an extension of work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}.
Instead of building a communication graph to custom fit the host topology, this work leverages the existing algorithm but renumbers the ranks behind the scenes to better map the communication graph to the hardware.
Their work focuses on broadcast and allgather collectives, with remapping algorithms targeting ring, recursive doubling and binomial communication patterns.
We extend their work by looking at allreduce and the additional challenges the reduction operation adds to their method.
We also propose mapping algorithms for reduce-scatter-allgather, knomial and binary-tree communication patterns.


\section{Method}
When applying topology awareness to collectives, there are two common strategies.
Either build a communication pattern that is designed to fit the hardware (this is what hierarchical algorithms do) or take an existing pattern and efficiently map it to the hardware.
This work uses the latter strategy, we take the collective algorithm selected by the implementation and silently renumber the processes to utilize communication resources better.
MPI providers will often include multiple algorithms for different collectives with common patterns, and each algorithm has a communication pattern which is implicit in the algorithm's design.
For example, ring only exchanges data with its immediate neighbours, and recursive doubling algorithms only exchange data with processes whose rank is a power of two distance away.
Since these patterns are pre-defined, we can extract a communication pattern and combine it with topology information before the collective is run.
This problem can be formalized as an instance of a graph-embedding problem, which previous work has shown to be an NP-Hard problem \cite{Hoefler2011GenericTopoMappingStrats}. 
So solving this problem at scale (on the order of millions of processes) is not feasible, therefore, heuristics and simplifications are often used instead.

Since we are interested in accelerating distributed deep learning, we target several allreduce algorithms, as well as a few broadcast algorithms, since they can be combined with a reduce to create an allreduce.
To make the calculation efficient and scalable, each proposed remapping heuristic leverages a greedy strategy.
An outline of the general strategy is given in algorithm \ref{alg:toporr-strat}.
The basic structure starts by binding rank zero to core zero and sets zero as the reference rank.
In the heuristics main loop, lines three to eight, all process remappings will try to bind to resources as close to the reference rank as possible.
While the heuristic loops through the ranks, it selects the next rank to map based on the communication pattern (line four) and updates the reference rank when required (line 7).

The mapping is enforced by creating a new communicator.
Each process calls \texttt{MPI\_Comm\_split()}, requesting their new rank, and then the selected algorithm is run on the resulting communicator.
Since communicator creation is an expensive operation, this shadow communicator is cached so that it can be used again if the original communicator uses the same algorithm.

This structure is flexible and allows us to evaluate multiple types of collectives.
Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} propose heuristics targeting ring allgather, recursive doubling allgather, and binomial tree broadcast. 
We reimplement their ring and recursive doubling algorithms and evaluate them on allreduce collectives, and evaluate their binomial-broadcast heuristic in the context of a scatter-allgather broadcast.
To extend their work while targeting large message allreduce, we propose a new reordering heuristic for reduce-scatter-allgather allreduce.
Furthermore, we also propose heuristics for two broadcast algorithms, knomial and binary tree broadcast.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/TopoRR_strat_pseudocode}

\subsection{Reduce-Scatter-Allgather Allreduce}
Reduce-scatter-allgather (RSA) allreduce, also known as Rabenseifner's algorithm, is a popular algorithm implemented in most MPI implementations and even used in external collective libraries like UCC \cite{UCC}.
The algorithm starts with each process performing a recursive-vector halving reduce-scatter, this distributes a segment of the fully reduced vector to each rank.
At this point, each rank needs to gather the other reduced segments from the other ranks, and this is achieved through a recursive doubling allgather.
RSA is used for large message allreduce because it efficiently uses system bandwidth.
For an allreduce with $p$ processes on $n$ bytes of data, each rank sends $2((p-1)/p)n\beta$ bytes of data, linear scaling with message size is ideal for large message collectives.
In terms of structure, since both phases rely on a recursive pattern, data is only exchanged with ranks that are a power of 2 distance away.
The other important feature of RSA is that more data is exchanged with numerically closer ranks.
Figure \ref{fig:graph-rsa} demonstrates this phenomenon with eight processes.
Most data is exchanged with immediate neighbours, this accounts for the first stage of reduce-scatter and the last stage of the allgather.

So while designing our heuristic, we want to map communicating pairs that are a power of two distances away, focusing on small distances first.
With these goals in mind, we propose the heuristic in algorithm \ref{alg:rsa} to target the RSA algorithm.
This heuristics starts by mapping rank 0 (line 1), then walks to the power of 2 communication graph mapping processes that have not been mapped yet (lines 5-10). 
While looping through all the ranks, the reference rank is updated every two mappings (lines 11-14), this ensures that communicating pairs that are distance one away are mapped as close as possible. 

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/Topo_graph_rsa}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/rsa_pseudocode}

\subsection{Binary Tree Broadcast}
Binary trees benefit from their simplicity.
Ranks receive one message and forward it to two child ranks.
The tree structure provides logarithmic scaling to the number of messages sent, so binary trees are often used for smaller message sizes. 
A typical structure for a binary tree is provided in Figure \ref{fig:graph-bin-tree}.
For a rank $r$, the height in the tree can be calculated as $h_r = \lfloor log_2(r+1) \rfloor$, and ranks will send messages to $r + 2^{h_r}$ and $r + 2^{h_r + 1}$
This leads to a structure where processes further down the tree send messages to ranks further and further away.
Furthermore, binary trees are constructed slightly lopsided,  as nodes are added to the left side of the tree before the right side. 
This can lead to situations where one direction has slightly more nodes than the other, and this should be accounted for when designing the heuristic.

To ensure that ranks reside on the same node as their children, we propose a depth-first-traversal-based heuristic with the code provided in algorithm \ref{alg:bintree}.
We employ a recursive function to traverse the tree efficiently. 
We start by mapping the tree's root (line 1) and passing it in as the reference rank to the recursive function (line 2).
The recursive function starts by calculating the reference rank's height and child processes (lines 4-7). 
Then, depending on if the child processes are in the communicator, we bind them close to the reference rank and recuse with the new process set as the reference rank (lines 8-15).
To account for lopsidedness, the heuristics traverses the potentially smaller side first by mapping the numerically greater child first.
We found that this does a more consistent job of mapping the tree to the system architectures we evaluated.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/Topo_graph_bintree}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/bin_tree_pseudocode}

\subsection{Knomial Broadcast}
Knomial is a generalization of a binomial tree, and its structure can be defined inductively.
For a knomial tree $T_{k,h}$ of height $h$ and radix $k$, if $h=0$, then $T_{k,h}$ is a single node. For $h>0$, $T_{k,h}$ is built from $k$ copies of $T_{k,h-1}$, where the root of the first copy is the parent node of the other $k-1$ copies.
Figure \ref{fig:graph-knomial} provides an example of a knomial tree with radix 4 and 16 processes.
While any value of $k$ can be used, implementations commonly use either 2 (binomial tree) or 4 as default values. 
While the tree's structure is heavily lopsided, in theory, this provides the opportunity to overlap message transfers at lower layers of the tree with transfers higher in higher layers.
Due to the inductive definitions, high $k$ value trees will have a lot of leaf nodes, as each subtree will contain $k-1$ subtrees with $h=0$.

In algorithm \ref{alg:knomial}, we propose another depth-first traversal-based heuristic, which focuses on mapping smaller subtrees first.
By focusing on subtrees we are trying to group bunches of leaf nodes on the same node as their parent.
Since this is a depth-first traversal, we use another recursive function to traverse the tree. 
Once again, we start by mapping the tree's root (line 1) and passing it in as the reference rank of the recursive function (line 2).
In the recursive function, we immediately map the $k-1$ subtrees of height 0 (lines 4-6).
Then we use a while loop to calculate the values for subtrees that are further away (lines 8-19).
If the root of a subtree does exist in the communicator, we map it as close as possible to the reference rank (line 14), calculate its depth (line 15), and recurse on it as the reference rank (line 16).

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/Topo_graph_knomial}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/knomial_pseudocode}

\subsection{Hessam's work on Binomial, Ring, and Recursive Doubling}

Previous work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} proposed a series of algorithms targeting recursive doubling allgather, ring, binomial broadcast, and binomial gather. 
While their work is evaluated in the context of allgather, the reordering heuristics they propose can also be used to accelerate allreduce. 

The ring and recursive doubling allgather heuristics can be transplanted directly to ring and recursive doubling allreduce, respectively, and the intuition behind both algorithms' utility holds. 
The ring targets nearest neighbour ranks, it starts at rank 0 and walks through the communicator mapping $r+1$ as close as possible to reference rank $r$, then setting $r+1$ as the new reference rank.
Since both ring allreduce and ring allgather only communicate with immediate neighbours (i.e. rank $r+1$ and $r-1$), the intuition of this heuristic is portable across collectives.
The recursive doubling algorithm targets the last two stages of the algorithm, as message sizes are the largest during those phases. 
It's similar to our proposed RSA heuristic but focuses on the last rounds of communication instead of the first. 
Recursive doubling allreduce and allgather share the same communication pattern, but while the message sizes double each round in allgather,  it remains fixed in allreduce.
So migrating the heuristic to allreduce might not see the same performance gains, but it will ensure that most communications take an efficient path.

We also evaluated the previously proposed recursive doubling allgather heuristic in the context of the scatter-allgather broadcast.
Scatter-allgather, also known as Van de Geijn's algorithm, is often used for large message broadcasts as it is incredibly bandwidth efficient, 
This is achieved by splitting the message up into segments, scattering the segments across the communicator with a binomial algorithm, and then reconstructing the message with an allgather of the segments.
The recursive doubling allgather has already been studied in \cite{Mirsadeghi2016TopoAwareCollRR}, it has an even communication pattern where the last stages of the algorithm have the largest message sizes.
On the other hand, the binomial scatter is similar to the binomial broadcast algorithm, with the difference that the message size halves between stages. 
The communication graph generated by the binomial pattern is essentially a subgraph of recursive doubling, so it can be layered on top of recursive doubling without adding any new communication dependencies. 
Therefore, we applied the recursive doubling reordering heuristic to scatter allgather broadcasts.

\section{Experiments and Analysis}
We evaluated our new heuristics, as well as the existing heuristics, on CPU and GPU allocations across two different clusters.
To ensure the robustness of our algorithm, we evaluate different initial mapping to stress test different starting conditions.
Furthermore, to evaluate against more traditional topology mapping methods, we compare our heuristics against SCOTCH, a commonly used graph partitioning library previously applied to topology-aware works.
% Through microbenchmark evaluation, we are able to demonstrate that our method can improve allreduce performance, and proper algorithm selection could lead to more-optimal allreduce perforamce.

\subsection{Software Implementation}
We implemented our method by modifying OpenMPI v4.0.5 \cite{gabriel2004OpenMPI}.
Building on top of OpenMPI let us leverage the existing collective implementations as well as the algorithm selection mechanism.
OpenMPI also has internal support for Hwloc, which makes gathering topology information easier.

There are some caveats with collective rank reordering as not all collectives can support it seamlessly.
By default, \texttt{MPI\_Allreduce()} algorithms assume operations are commutative, but users can specify operations to be non-commutative with respect to process's ranks. 
To support non-commutative operations, Allreduce algorithms must be carefully designed to ensure operations are applied in the correct order, but renumbering processes break that careful design.
Therefore, this work only supports commutative operations.
Furthermore, both broadcast heuristics assume rank 0 is the root which is not always true. 
So when rank 0 isn't the broadcast root, it needs to receive the data from the real root before it can start the broadcast on the shadow-communicator.
(In retrospect, a smarter design would have been to assign virtual ranks by shifting each rank according to the root's value, this would make the root 0, but I last looked at the code for remapping a year ago, and the performance difference provided by this step would be negligible; furthermore, OMB only calls bcast w/ root 0, and Horovod rarely uses bcast.)

We leverage existing system topology detection tools to gather topology information for both the node-local topology as well as the network topology.
Inra-node hardware info was gathered through Hwloc \cite{Broquedis2010hwloc}, a tool that repackages information provided by the operating system and presents it in a user-friendly way.
Hwloc is a run-time library that can efficiently provide information on NUMA domains and cache hierarchies during job execution.
Thankfully, Hwloc is system agnostic as it mainly relies on endpoints standardized by the operating system, but conversely, network monitoring/management tools are vendor specific.
To extract the network topology, we leveraged ibnetdiscover for InfiniBand and Opareport for OmniPath, both tools scan the entire network and require admin privileges to run, so integrating them at runtime is not a viable option.
Both tools generate text files describing how network ports are connected, along with other info like speed and link status.
Since the tools have different outputs, they were transformed into a common format and saved to disk.
Topology information is represented as a matrix at runtime, and each node is responsible for generating the row corresponding to its rank.
Within each row, values represent the cost of sending data to other ranks, and an allgather is used to combine all processes' node information and generate the final results.
(This method is not that scalable as the memory requirements are on the order of $O(n^2)$ with respect to the number of processes, but it is viable for the scale we're running at.)

OpenMPI does support Allredce with CUDA memory, but their implementation is fairly naive.
When allreduce is called on GPU memory, the data is copied to a temporary host buffer, then a CPU-based allreudce is performed, and the final result is copied back up to the GPU.
While this does fulfill the MPI specification, it neglects to use the available GPU hardware like compute kernel reduction or GPU to GPU data transfers with NVLinks.
So we modified OpenMPI's Allreduce algorithms to make use of GPU hardware. %, this is also more in line with how modern collective libraries like NCCL and UCC implement allreduce \cite{UCC, NCCL}.

The rank reordering strategy finds performance improvements by rearranging the process to core bindings, which implies that any performance improvements are dependent on the initial process to core bindings.
MPI implementations provide flags for users to specify how processes should be distributed at job launch. 
While not exhaustive, this interface does provide a starting point for users to optimize the process to node mappings.
Our method can calculate more complicated mappings, as well as mappings for subsets of processes when an application creates new communicators.  

In OpenMPI, users can specify a resource and processes are bound to that resource in a round-robin fashion.
Figure \ref{fig:init-mappings} provides an example outlining the three initial mappings we evaluated.
The default initial mapping is by-package, this takes a node and fills it with processes alternating between packages until the node is full, it then repeats this on the next node until all processes are bound.
By-core mapping places processes to consecutively numbered cores filling up a node before moving to the next, and by-node scatters processes across nodes by cycling through nodes and placing processes one at a time.

To compare our proposed heuristics against an established topology mapping tool, we also integrated the SCOTCH graph embedding tool \cite{Pellegrini2012SCOTCH} into the rank reordering method.
SCOTCH uses a graph bisecting method to perform graph partitioning and embedding, and it has been used in previous work to incorporate topology awareness \cite{Mirsadeghi2016TopoAwareCollRR}.
Out of the box, OpenMPI relies on a static tuning table to choose a collective algorithm based on the communicator size and message size, so we applied SCOTCH to whichever algorithm the tuning table selected.
Due to technical and timeline-related reasons, we could only evaluate SCOTCH on Beluga.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/init_mapping}

\subsection{Hardware}\label{sec:CH4-eval-hardware}
We evaluated our work on two heterogeneous clusters, Beluga and Cedar, both provided by Compute Canada. 
Beluga is Infiniband based, structured as a 5:1 blocking fat tree with static routing.  
The CPU nodes have dual socket Xeon Gold 6148 Skylake processors, and the GPU nodes use the same CPUs attached to four V100s fully connected with NVLink.
Cedar, on the other hand, is OmniPath based, it also has a fat-tree topology configured, but it is configured with 2:1 blocking and has adaptive routing.
Cedar's CPU allocation has dual-socket 24-core Xeon Platinum 8260 Cascade Lake processors, while the GPU nodes have two 20-core Xeon Silver 4216 Cascade Lake processors hosting four v100s GPUs fully connected by NVLink.
The key difference between Beluga and Cedar's GPU nodes is the PCIe connections between CPUs and GPUs.
On Beluga, all the GPUs are connected to socket 0 through a PCI switch, while Cedar has GPUs split between sockets 0 and 1, each connecting to the socket.

\subsection{Results}

\subsubsection{Performance}

We used OSU-Microbenchamrks v5.7 \cite{Bureddy2012OMB} to evaluate the performance of our proposed method. 
The microbenchmark loops over \texttt{MPI\_Allreduce()} measuring the runtime of each function call and outputs the average latency. 
It also provides the option to locate the data buffer in either CPU or GPU memory.

GPU allocations were evaluated with four processes per node (since there are only 4 GPUs per node), and CPU allocations were evaluated with 32 processes per node because that is the largest power of two that can fit on a node without oversubscribing resources.
We present the data for large jobs on Beluga, using 128 nodes at a time for both CPU and GPU partitions, this gives us 512 GPU processes total and 4096 CPU processes total.
The data on Beluga for our Allreduce experiments are provided in figure \ref{fig:beluga-ar-128}, and broadcast results are in figure \ref{fig:beluga-bc-128}.
Cedar's allocations were a bit smaller, with 32 GPU nodes (communicator size of 128) and 64 CPU nodes (communicator size of 2048).
The Allreduce results on Cedar are presented in figure \ref{fig:cedar-ar-32g64c}, and the Broadcast results are in figure \ref{fig:cedar-bc-32g64c}.
While results on Cedar are not identical to Beluga due to differences in computer architecture and experiment setup, the benefits of our proposed methods are consistent across both clusters.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/ar_beluga_128}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/bc_beluga_128}

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/ar_cedar_32g_64c}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/bc_cedar_32g_64c}

Across all evaluated examples, at least one of the remapped algorithms performs better, or on par, with the default mapping. 
There would be an opportunity to restructure OpenMPI's tuning table to select the most optimal remapped algorithm, providing a more performant \texttt{MPI\_Allreduce()}.
There are scenarios where SCOTCH can improve performance, like small message GPU allreduce, but it is most often the case that it negatively impacts performance.

For allreduce, the most impactful results are CPUs remapped from by-node.
Beluga can see a 60\% improvement over the default for RSA and as high as 74\% at certain sizes like 512kB.
Cedar's CPU allreduce has an even greater improvement with 85\% for 64MB.
The GPU allreduce on beluga doesn't see that much improvement, but the results on cedar are strong, with 64MB ring seeing 68\% improvement, 64MB RSA seeing 54\% improvement, and RSA seeing 90\% improvement at 8MB and 16MB.

The broadcast results are a bit more varied.
For GPU resources, the best results are seen on a by-node mapping.
The by-node results on Beluga's GPUs ran into message size scaling issues, but the largest size we could collect (8MB) had a 70\% improvement over the default, and while not as extreme, Cedar still sees 42\% improvement for 64Mb on a by-node mapping.
Interestingly, the CPU results tend to prefer the by-socket and by-core mappings. 
The knomial and binary tree results both see 80\% performance improvements for all large messages on Beluga, and the binary tree remapping sees a 51\% improvement on the same initial mappings on Cedar.

Often, the benefits of remapping is noticed with a by-node initial mapping. 
For example, in figure \ref{fig:cedar-ar-32g64c}, ring allreduce on GPUs has can beat the default by 3\%-7\% for large messages on by-socket and by-core mappings but sees a 30\%-60\% improvement for the same message range on a by-node mapping.
OpenMPI's collective algorithms tend to be structured so that processes exchange most of their data with nearby ranks, this design characteristic is most pronounced in algorithms like RSA, ring, and knomial.
This design decision tends to perform well on by-socket and by-core mappings, which reflects in our results where default OpenMPI performs similarly to many of our remapped algorithms on by-core and by-socket initial mappings.
But, the same algorithms tend to struggle with by-node initial mappings because each process's nearby ranks are off-node, which leads to a lot more data traversing the network.
The binary-tree mapping is an exception to this pattern as child ranks are often much further away than parent ranks, therefore binary tree remapping has much more potential than other algorithms.
The topology awareness of our remapping method solves this issue and relieves network pressure by moving most of the communication back within the node and tends to outperform the default by a large margin.

The impact of remapping is much more noticeable on CPU resources than on GPUs.
For example, in figure \ref{fig:beluga-ar-128} reordering large message RSA on CPUs mapped by-node sees around 60\% improvement over the default, while the same algorithm only sees 15\% on GPUs.
This can be justified through topology awareness' ability to relieve contention on resources like network cards or inter-socket links. 
Both the CPU and GPU nodes have a single network card, but the CPU benchmarks have 8x more processes per node than the GPU jobs, creating a lot more demand for the network card on each node.
Further, CPU jobs have to manage contention for the inter-socket link, the GPU architecture avoids this issue since all the cards are fully-connected.
While both allocations benefit from remapping, the results are more impactful for the CPU jobs as they can redirect much more of the resource demand.

\subsubsection{Scalability}

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/beluga_scale}

The remapping method does retain its performance wins across communicator sizes. 
Figure \ref{fig:beluga-scale-64m} demonstrates how each algorithm compares scaling a 64MB collective from 32 nodes to 128 nodes.
Only a by-socket mapping is presented, but by-core and by-node have similar scaling characteristics.
Some of the heuristics are still hampered by their underlying algorithm, recursive doubling being the worst offender, but the performance improvements scale well across the board.

\subsubsection{Applications}

We also evaluated the effects of remapping allreduce on data-parallel DL training, but there was no obervable impact.
We tested a series of models using Synthetic datasets accross CPU and GPU resources on Beluga and Cedar, we also evaluated the same set of initial mappings, but there was no disernable difference in performance between remapped and non-remapped collectives.
In the GPU microbenchmarks large message allreduce doesn't see much improvement from remapping, even with a by-core initial mapping, and that is reflected in the model training performance.
The lack of performance difference in CPU training is because that benchmark is not bound by allreduce, it's spending most of it's time in CPU compute.
So while microbenchamrks do show that a topology-aware reammapping can improve allreduce performance, the benifits are not present in the message sizes used for data-parallel DL training with Horovod.

\section{I can't believe it's not a conclusion â„¢}
This chapter expands on the work proposed by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} by expanding their idea to new collective communication algorithms.
We show that their methods can be applied to RSA, ring, and recursive doubling allreduce, along with binary-tree, knomial, and scatter-allgather broadcast.
It works across different initial mappings but sees the best results on a by-node mapping.
While this work is best applied to dense CPU allocations, it can be applied to GPU allocations as well. 
While not explicitly demonstrated, since there is consistently one algorithm that performs on par or better than the default, we could theoretically build a tuning table to provide better overall \texttt{MPI\_Allreduce()} performance. 

The idea of this work was to accelerate allreduce through the topology-aware algorithms, with the motivation of decreasing deep learning training time.
Topology awareness has been exhaustively evaluated by many researchers, with a plethora of ideas existing in the literature.
But, topology awareness is not the only strategy for accelerating allreduce.
In the next chapter, we tackle the same problem of accelerating allreduce but from the angle of process arrival pattern awareness.
This is still in the domain of large-message allreudce, so many existing algorithms are applicable, but there are numerous differences and new challenges which require clever and innovative ideas to solve.
