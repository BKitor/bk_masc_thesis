% Chapter 4 - Topology Awareness

\glsresetall % reset the glossary to expand acronyms again
\chapter[Topology]{Topology Awareness}\label{ch:TopologyAwareness}
\index{Topology Awareness}

% Topology Awareness

\begin{itemize}
    \item Background on Topology-Awareness
    \begin{itemize}
        \item Graph mapping of communication pattern to host topology
        \item is an NP-hard problem, most-often heuristics are used
        \item existing methods to apply topo-awareness
        \begin{itemize}
            \item general \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016TopoAwareCollRR, Mirsadeghi2016MAGC}
            \item comm-topo \cite{Gropp2019CartTopoMapping}
            \item collectives \cite{Mercer2011ImprvMPIWithRR, Mirsadeghi2016TopoAwareCollRR}
        \end{itemize}
    \end{itemize}
    \item Reordering and Caching method
    \item Reordering algs for RSA, Binary-tree, Knomial-tree
    \item CPU vs GPU kernel
    \item System topology using hwloc and ibnetdiscover
    \item Evaluation
    \begin{itemize}
        \item Microbenchmark Data
        \item SCOTCH \cite{Pellegrini2012SCOTCH}
        \item Horovod, even though there's barely any improvement?
    \end{itemize}
\end{itemize}

The first technique investigated to improve MPI collective communication performance is topology awareness.
The overarching idea is to accelerate computation by leveraging the knowledge of the underlying hardware.
Topology awareness is an often-used technique applied to many areas in both MPI and the greater HPC ecosystem.

This chapter builds on work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}, where the authors propose a method for applying topology awareness to allgather and broadcast.
Their work relies on the notion that collective algorithms have an implicit communication pattern, and that the ranks in a communicator can be reordered as to better fit the communication pattern to the host topology.
We start by extend their work to multiple new algorithms, in allreduce and broadcast.
Microbenchmark evaluation shows that we can see up to 80\% performance improvement under certain scenarios, and that we can outperformed the SCOTCH graph partitioning library \cite{Pellegrini2012SCOTCH}.

This chapter will start by outlining existing methods for applying topology awareness within MPI.
Next, we propose a method of accelerating MPI collectives and build an implementation targeting multiple MPI\_Allreduce and MPI\_Bcast algorithms.
Our work is evaluated in both CPU and GPU environments and can see up to 80\% improvement in certain scenarios.

\section{Motivation}
MPI provides a programming model so that processes on a distributed memory system can share data and work together.
MPI users are made to assume that any messages exchanged between any two ranks will have the same communication characteristics no matter what their ranks are. 
This abstraction provides a convenient programming environment allowing users to focus on building their application without worrying about the underlying hardware. 
Its simplicity also has the benefit of making portability easier, dissuading users from tying their application to a specific architecture.
But in reality, this assumption is false because modern computers are anything but simple. 
Compute nodes have expansive memory hierarchies spanning multiple caches, NUMA domains, and even GPU memory.
Furthermore, modern fabric interconnects have complex network topologies with variable performance depending on node location.
So, in reality, the hardware topology of the system has an outsized impact on message performance, which has a knock-on effect on overall application performance. 

MPI applications often have consistent and predictable data-transfer patterns, this means anyone rank will send and receive the same set of messages to and from the same set of peers on consecutive runs of the application.
Since data-transfer patterns are predictable, and the programming model doesn't bind processes to specific locations, this gives MPI implementations the flexibility to map ranks to processing elements to make the most optimal use of the underlying hardware.
This problem of mapping ranks to processing elements can be formalized as an instance of a graph-embedding problem. However, previous work has shown this to be an NP-Hard problem \cite{Hoefler2011GenericTopoMappingStrats}. 
Solving this problem at scale (on the order of millions of processes) is not feasible. Therefore heuristics and simplifications are often used instead.

To apply topology awareness to collectives, we can leverage how they are structured as a series of point-to-point communications.
The algorithmic implementation of a collective specifies a pattern of data exchanges, which can be interpreted as a communication graph.
MPI providers will often include multiple algorithms for different collectives with common patterns, including ring, recursive doubling, knomial-tree, etc...
This gives MPI implementations the flexibility to select a performant algorithm depending on the collective's parameters like message size and the number of processes.
This framework for algorithm selection neglects the host topology, but there is room to incorporate it.
The collective algorithm defines the pattern to disseminate data, but the value of the ranks in the graph often  does not matter (there are some algorithms where this does matter, like allgather, alltoall and non-commutative allreduce, but these can be accounted for).
So in this work, we evaluate the efficacy of mapping collective communication graphs to the underlying hardware topology on-the-fly during application runtime.


% Conseptualy, this problem can be treated as a graph mapping problem. 
% The MPI application is modeld as weighted graph $G=(V_G, w_G)$, where verticeis are processes and weights are the amount of communication between processes.
% The host topology is also models as a weighted graph $H=(V_h, w_H)$, where vertices are processing elements and weights represent the capacity of the interconnect between any two processing elements.
% % $G$ and $H$ are both fully connected graphs, and $|V_G| = |V_H|$.
% When running the application, the grpah $G$ is mapped to $H$ so that each rank in $V_G$ is overlayed on a processing element in $V_H$, and communication weights in $w_G$ are tied to a hardware link $w_H$.
% Lastly, a mathematical metric is defined to estimate the performance of the mapping.
% So the goal of topology-awareness is to find a mapping from $G$ to $H$ that minimized/maximized a defined metric.
% This problem, which is a subset of graph-embedding, is NP-hard, so finding optimal solutions for large scale instances of this problem is not feasable \cite{Hoefler2011GenericTopoMappingStrats}.
% Solutions used in practice often rely on heuristics to find near-optimal solutions in a reasonable amount of time.


\section{Background}
Topology awareness is a commonly used technique for accelerating MPI applications. 
Within MPI, there are many scenarios and methods where topology information can be applied to accelerate communication.
At the highest level, topology awareness can be applied to the application's entire communication graph \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016PTRAM, Faraji2016TopoAwareGPUSelection, Mirsadeghi2016MAGC, Galvez2017AutoTopoMap}.
These strategies require profiling the entire application to build the communication graph.
This communication graph is then used in future runs of the application to devise a mapping for each job allocation.
The efficiency of a mapping is evaluated using metrics such as hop-bytes or congestion.
So to achieve the best application performance, mapping algorithms try to minimize/maximize their chosen metric.
There are multiple research fronts with this strategy, efforts have gone into optimizing the metrics, the mapping algorithm itself, and the types of systems mappings are supported for.

Hoefler and Snir \cite{Hoefler2011GenericTopoMappingStrats} propose a general process mapping tool targeting CPU clusters.
They evaluate three algorithms, a greedy algorithm based on vertecie weights, a recursive bisecting that makes minimum weighted edge-cuts, and a graph similarity mapping using the Reverse Cuthill McKee algorithm. 
They use these algorithms, along with a \textit{Threshold Accepting} optimization step, to minimize congestion and dilation on large-scale SMP clusters.

Mirsadeghi and Afsahi \cite{Mirsadeghi2016PTRAM} propose a system targeting large-scale Infiniband clusters.
Their system leverages the network topology plus Infiniband's static routing tables to further reduce congestion.
They propose a hybrid metric which is a linear combination of hop-bytes and three types of congestion statistics. 
The mapping, along with further refinements, are calculated using a parallel greedy algorithm.

Faraji et al. l \cite{Faraji2016TopoAwareGPUSelection} focus their efforts on building a system targeting intranode GPU communication.
While their system only works on a single node and relies on SCOTCH's \cite{Pellegrini2012SCOTCH} graph bisecting method to perform the mapping, Mirsadehi et al. \cite{Mirsadeghi2016MAGC} expand the work to a full cluster.
The complete system uses a 3-step process, first mapping ranks to nodes to minimize network communication, then ranks to core to optimize intranode communication, and lastly, the GPU-to-rank step for optimal GPU-to-GPU communications.

Existing systems struggle to manage collective communications.
The profiling stage is often built on top of the PMPI profiling interface, which can intercept MPI calls, but doesn't break collectives into their constituent point-to-point messages.
Galvez et al. \cite{Galvez2017AutoTopoMap} identify this problem and propose a profiler that groups types of communications into weighted classes.
Their system leverages a parallel algorithm that uses the communication classes, along with a set of weighted metrics, to calculate a near-ideal mapping.
While a step in the right direction, their solution still treats collective communications as a black box, focusing on the collective's communicator and not disassembling the collective into its constituent point-to-point messages.

In order to decompose collectives into point-to-point messages, Bosilica et al. \cite{Bosilica2017OnlineMonitoringMPI} implemented a monitoring layer in OpenMPI that is accessible through the MPI Tools interface.
This provides much more granular profiling for point-to-point messages, which allowed Jeannot and Sartori \cite{Jeannot2020ImprvMPICommMonitoring} to propose a rank reordering method for applications with iterative compute, but their solution is required to be implemented within the user's application. 
So while it is possible to apply topology awareness to all communication, including collectives, there is a large burden on application developers to implement it within their software.

One of the common shortcomings most of these systems have in common is the required application profiling.
The application needs to run at least once to build the communication graph, and this can be expensive for large-scale runs. 
An ideal solution would be able to build a graph for a moderate amount of processes and project a solution for a large-scale system, but no existing solution can perform this yet. 

The MPI standard does provide a few interfaces to allow application developers to inform the runtime of expected communication patterns.  
One of these solutions are virtual topologies. 
This is a structure attached to a communicator that tells the implementation how communications are likely to occur between ranks.
Mercer and Jeannot \cite{Mercer2011ImprvMPIWithRR} leverage the MPI\_Dist\_graph\_create endpoint to reorder processes on a graph topology.
Their solution centralizes the topology information and uses the TreeMatch algorithm \cite{Jeannot2010TreeMatch} to calculate a process reordering.
Gropp \cite{Gropp2019CartTopoMapping} proposes a method for reordering processes in a cartesian topology.
His method relies on MPI\_COMM\_TYPE\_SHARED and neglects network topology as well as memory hierarchy information.

The most common way to apply topology awareness within collectives is to use a hierarchical strategy.
This is done by splitting the communicator into intra-node and inter-node sub-communicators.
The intra-node communicator consists of processes that are all placed on the same node.
Each node selects a leader process which is a member of the inter-communicator.
The collective communication pattern is then implemented across the hierarchical structure, ensuring efficient hardware use between shared memory and network resources.

Lue et al. \cite{Luo2018ADAPT} propose a library for event-based collectives.
While the crux of their work is focused on minimizing synchronization dependencies within collectives, they do demonstrate how their work could easily be mapped to a hierarchical type algorithm.
Awan et al. \cite{Awan2016NCCLBcast} build a GPU-aware hierarchical bcast algorithm.
Their work leverages NCCL for optimal intra-node communication for large messages and relies on existing algorithms in MVAPICH2 for inter-node communication.

Subramoni et al. \cite{Subramoni2011SpeedAwareBcast} designed a broadcast algorithm that can take Infiniband topology and network speeds into account.
They gather network information using OFED management tools like ibnetdiscover and ibroute and use the information to build a reordered communicator using either a depth-first traversal or a breath-first traversal.
They only evaluate knomial and scatter-allgather collectives.

The work presented in this thesis is an extension of work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}.
Instead of building a communication graph to custom fit the host topology, this work leverages the existing algorithm but renumbers the ranks under the hood to better map the communication graph to the hardware.
Their work focuses on broadcast and allgather collectives, with remapping algorithms targeting ring, recursive doubling and binomial communication patterns.
We extend their work by looking at allreduce and the additional challenges the reduction operation adds to their method.
We also propose mapping algorithms for reduce-scatter-allgather, knomial and binary-tree communication patterns.


\section{Method}

When applying topology awareness to collectives there are two common stategies.
Either build a communication pattern that is designed to fit the hardware, or take an existing pattern and map it to the hardware.
This work uses the latter strategy, we take the collective algorithm selected by the implementation and silently renumber the processes so as to better adapt it to the hardware.
When a collective is called, the MPI library selects an algorithm based on the message size and number of processes.
The communication pattern is implicit to the collective algorithm, so we define a heuristic tailored to each algorithm that calculates how ranks should be renumbered.
To make the calculation efficient and scalable each proposed remapping heuristic leverages a greedy strategy, an outline of the general strategy is given in Listing \ref{lst:topo-generic-strategy}.
The basic structure starts with a reference rank, all process remappings will try to use hardware as close to the reference rank as possible.
Then the heuristic loops through the ranks, selecting the next rank to map based on the communication patern, and updating the reference rank on occasion.

The mapping is enforced by creating a new communicator, each process calls MPI\_Comm\_split() and requests their new rank, and the specified algorithm is run on the resulting communicator.
Since communicator creartion is an expensive operation, this shadow communicator is cached so that it can be used again if the same communicator uses the same algorithm.

This structure is flexible, and alows us to evaluate multiple types of collectives.
In order to accelerate large message allreduce, we propose a reordering heuristic for reduce-scatter-allgather allreduce.
Furthurmore, since broadcast can be used as a component in allreudce, we also propose heuristics for two broadcast algorithms, knomial and binary tree broadcast.

\lstset{style = bklstc}
\lstset{label = lst:topo-generic-strategy}
\lstset{caption = General greedy heuristic for topology-aware rank reordering.}
\lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/TopoRR_strat.c}

\subsection{Reduce-Scatter-Allgather Allreduce}

Reduce-scatter-allgather (RSA) allreduce, also known as Rabenseifner's algorithm, is a popular algorithm implemented in most MPI implementations, and even used in external collective libraries.
The algorithm starts with each processess performing a recursive-vector halving reduce-scatter, distributing segments of the reduction vector accross all ranks.
After the reduce scatter, each rank will have a completly reduce segment of the final output, so a recursive doubling allgather is performed to gather the final result.
RSA is used for large message allreduce because it is bandiwth optimal.
For an allreduce with $p$ processes on $n$ bytes of data, each rank sends $2((p-1)/p)n\beta$ bytes of data.
Since both phases rely on a recusive pattern, data is only exchanged with ranks that are a power of 2 distance away.
The other important feature of RSA is that more data is exchanged with closer processes.
Figure \ref{fig:graph-rsa} demonstrates this phenomona with 8 processes.
The most data is exchanged with immediate neighbours, this accounts for the first stage of reduce-scatter and the stage of the allgather.


\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_graph_rsa}
\lstset{label = lst:topo-rsa}
\lstset{caption = Heuristic for rank reordering the reduce-scatter-allgather algorithm.}
\lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_RSA.c}

\subsection{Binary Tree Broadcast}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_graph_bintree}
\lstset{label = lst:topo-bintree}
\lstset{caption = Heuristic for rank reordering binary trees.}
\lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_BinTree.c}

\subsection{Knomial Broadcast}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_graph_knomial}
\lstset{label = lst:topo-knomial}
\lstset{caption = Heuristic for rank reordering knomial trees.}
\lstinputlisting[float=!htbp]{3_Chapters/4_Chapter_TopologyAwareness/Figures/Topo_Knomial.c}

\subsection{Implementation}

Built into OMPI v4.0.5
Intranode information with HWLOC
Internode info with ibnetdiscover/opareport -> python script to derrive distance matrix -> .txt file in home directory

\section{Results}

