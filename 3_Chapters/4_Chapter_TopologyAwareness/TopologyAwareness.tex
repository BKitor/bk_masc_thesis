% Chapter 4 - Topology Awareness

\glsresetall % reset the glossary to expand acronyms again
\chapter[Topology]{Topology Awareness}\label{ch:TopologyAwareness}
\index{Topology Awareness}

% Topology Awareness

\begin{itemize}
    \item Background on Topology-Awareness
    \begin{itemize}
        \item Graph mapping of communication pattern to host topology
        \item is an NP hard problem, most-often heuristics are used
        \item exsisting methods to apply topo-awareness
        \begin{itemize}
            \item general \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016TopoAwareCollRR, Mirsadeghi2016MAGC}
            \item comm-topo \cite{Gropp2019CartTopoMapping}
            \item collectives \cite{Mercer2011ImprvMPIWithRR, Mirsadeghi2016TopoAwareCollRR}
        \end{itemize}
    \end{itemize}
    \item Reordering and Caching Algorithm
    \item Reordering algs for RSA, Binary-tree, Knomial-tree
    \item CPU vs GPU kernel
    \item System topology using hwloc and ibnetdiscover
    \item Evaluation
    \begin{itemize}
        \item Microbenchmark Data
        \item SCOTCH \cite{Pellegrini2012SCOTCH}
        \item Horovod, even though there's barely any improvement?
    \end{itemize}
\end{itemize}

The frist thechnique investigated to improve MPI collectives performance is topology-awareness.
The overarching idea is to accelerate computation speed by leveraging knoledge of the underlying hardware.
Topology-awareness is often-used tequnique applided to many areas in both MPI and the greater HPC ecosystem.
After outlining existing methods for applying topology awareness within MPI, a method of accelerating MPI collectives is proposed, and implemented targeting multiple MPI\_Allreduce and MPI\_Bcast algorithms.
Our work is evaluated in both CPU and GPU environments, and can see up to 80\% improvement in certain scenarios.

\section{Modivation}
MPI provides a programming model so that processes on a distributed memory system can share data and work together.
MPI users are made to assume that any messages exchanged between any two ranks will have the same communicatoin characteristics no matter what their ranks are. 
This abstraction provdes a convinient programing environment allowing users to focus on building their application without worrying about the underlying hardware. 
The simplicity of it also has the benifit of making portability easier, disuading users from tying their application to a specific architecture.
But in reality, this assumption is false because modern computers are anything but simple. 
Compute nodes have expansive memory hierarchies spanning multiple caches, NUMA domains, and even GPU memory.
Furthurmore, modern fabric interconnects have complex topologies with variable performance depeding on node location.
So, in reality, the hardware topology of the system has an outiszed impact on message performance, which has a knock-on affect on overall application performance. 

MPI Applications tend to have consistant data-transer patterns, meaning ranks will most likely send and recive the same set of messages from the same ranks on consecutive runs of the application.
Since data-transfer patterns are predictable, and the programming model doesn't bind processes to specific locations, MPI implementations can try map ranks to processing elements as to make most optimal use of the underlying hardware.

Conseptualy, this problem can be treated as a graph mapping problem. 
The MPI application is modeld as weighted graph $G=(V_G, w_G)$, where verticeis are processes and weights are the amount of communication between processes.
The host topology is also models as a weighted graph $H=(V_h, w_H)$, where vertices are processing elements and weights represent the capacity of the interconnect between any two processing elements.
% $G$ and $H$ are both fully connected graphs, and $|V_G| = |V_H|$.
When running the application, the grpah $G$ is mapped to $H$ so that each rank in $V_G$ is overlayed on a processing element in $V_H$, and communication weights in $w_G$ are tied to a hardware link $w_H$.
Lastly, a mathematical metric is defined to estimate the performance of the mapping.
So the goal of topology-awareness is to find a mapping from $G$ to $H$ that minimized/maximized a defined metric.
The issue is, this problem is Is NP-hard, so finding optimal solutions for large scale instances of this problem is not feasable \cite{Hoefler2011GenericTopoMappingStrats}.
Solutions used in practice often rely on heuristics to find near-optimal solutions in a reasonable amount of time.

Collectives are structured as a series of point to point communications which can be interpreted as their own communication graph.
The structure of the graph is dependent on the underlying algorithm, i.e. ring, recursive doubling, knomial-tree, etc...
So in this work, we evaluate the efficacy of mapping collective communication graphs to the underlying hardware topology on-the-fly during application runtime.

\section{Background}
Topology-awareness is a commonly used technique for accelerating MPI applications. 
Within MPI, there are many scenarios where topology information can be applied to accelerate communication.
At the highest level, topology awareness can be applied to the entire communicatoin graph \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016PTRAM, Faraji2016TopoAwareGPUSelection, Mirsadeghi2016MAGC, Galvez2017AutoTopoMap}.
These strategies start by profiling the entire application to build a communicatoin graph for a specific instance of the application.
This communication graph is then used in future runs of the application to devise a mapping for each job allocation.
These strategies can varry on the metric used to evaluate mappings, the mapping algorithm itself, and the types of systems mappings are supported for.

Heofler and Snir \cite{Hoefler2011GenericTopoMappingStrats} propose a general process mapping tool targeting CPU clusters.
They evaluate 3 algorithms, a greedy algorithm based on vertecie weights, a recusive bisecting that makes minimum weighted edge-cuts, and a graph similarity mapping using the Reverse Cuthill McKee algorithm. 
They use these algorithms, along with a \textit{Threshold Accepting} optimizatoin step, to minimize congestion and dilation on large scale SMP clusters.

Mirsadeghi and Afsahi \cite{Mirsadeghi2016PTRAM} propose a hybrid metric

\section{Method}
\section{Results}
