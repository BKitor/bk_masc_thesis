% Chapter 4 - Topology Awareness

% \glsresetall % reset the glossary to expand acronyms again
\chapter[Topology Aware Allreduce and Broadcast for Heterogenous Clusters]{Topology Aware Allreduce and Broadcast for Heterogenous Clusters }\label{ch:CH4-TopologyAwareness}
\index{Topology Awareness}

The first technique leveraged to improve \gls{MPI} collective communication performance is topology awareness.
The overarching idea is to accelerate communication by leveraging the knowledge of the underlying hardware.
Topology awareness is an often-used technique applied to many areas in both \gls{MPI} and the greater \gls{HPC} ecosystem.

In \cite{Mirsadeghi2016TopoAwareCollRR}, Mirsadeghi and Afsahi propose topology aware collective rank reordering for allgather.
Their work relies on the notion that collective algorithms have an implicit communication pattern and that the ranks in a communicator can be reordered to better fit the communication pattern to the host topology.
We propose extending this methodology to multiple new algorithms in allreduce and broadcast.
We see up to 80\% performance improvements under certain scenarios in micro-benchmark study, and our proposed mapping heuristics can outperform established techniques.

This chapter will start by motivating the usage of topology awareness and outline existing methods using topology awareness within \gls{MPI}.
Next, we propose a methodology to accelerate \gls{MPI} collectives and build an implementation targeting multiple \texttt{MPI\_Allreduce()} and \texttt{MPI\_Bcast()} algorithms.
Our work is evaluated in \gls{CPU} and \gls{GPU} environments on two different clusters.

\section{Motivation}
\gls{MPI} provides a programming model for processes to exchange data across distributed or shared memory and efficiently work together.
\gls{MPI} users are meant to assume that messages exchanged between any two ranks will have the same communication characteristics no matter what.
This abstraction provides a convenient programming environment allowing users to focus on building their application without worrying about the underlying hardware. 
Its simplicity also has the benefit of making portability easier, dissuading users from tying their application to a specific architecture.
But in reality, this assumption is false because modern computers are anything but simple. 
Compute nodes have expansive memory hierarchies spanning multiple caches, \gls{NUMA} domains, and even \gls{GPU} memory.
Furthermore, modern fabric interconnects have complex network topologies with variable performance depending on node location.
So, in reality, the hardware topology of the system has an outsized impact on message performance, which has a knock-on effect on overall application performance. 

Performance improvements with topology awareness involve combining topology information with communication information.
We have tools for collective topology information at runtime, and the collective's communication pattern is implicit to whichever algorithm they use.  
Different algorithms (e.g., ring, recursive doubling, binary tree, binomial tree) have predefined data-transfer patterns, we know ahead of time how ranks will send and receive messages, and we can trivially model these relationships for different sizes of communicators.
There are existing methods for topology-aware collectives, like hierarchical collectives, but they tend to add extra communication steps and complexity than standard flat algorithms.
So this work evaluates and extends an existing method for adding topology awareness to flat algorithms by rank reordering.

\section{Related works}
Topology awareness is a commonly used technique for accelerating \gls{MPI} applications. 
Within \gls{MPI}, there are many scenarios and methods where topology information can be applied to accelerate communication.
At the highest level, topology awareness can be applied to the application's entire communication graph \cite{Hoefler2011GenericTopoMappingStrats, Mirsadeghi2016PTRAM, Faraji2016TopoAwareGPUSelection, Mirsadeghi2016MAGC, Galvez2017AutoTopoMap}.
These strategies require profiling the entire application to build the communication graph.
This communication graph is then used in future runs of the application to devise a rank-to-core mapping for each job allocation.
The efficiency of a mapping is evaluated using metrics such as hop-bytes or congestion.
So to achieve the best application performance, mapping algorithms try to minimize/maximize their chosen metric.
There are multiple research fronts with this strategy, and efforts have gone into optimizing the metrics, the mapping algorithm itself, and the types of systems mappings can target.

Hoefler and Snir \cite{Hoefler2011GenericTopoMappingStrats} propose a general process mapping tool targeting \gls{CPU} clusters.
They evaluate three algorithms, a greedy algorithm based on vertex weights, a recursive bisecting that makes minimum weighted edge-cuts, and a graph similarity mapping using the Reverse Cuthill McKee algorithm. 
They use these algorithms and a \textit{Threshold Accepting} optimization step to minimize congestion and dilation on large-scale \gls{SMP} clusters.

Mirsadeghi and Afsahi \cite{Mirsadeghi2016PTRAM} propose a system targeting large-scale InfiniBand clusters.
Their system leverages the network topology plus InfiniBand's static routing tables to further reduce congestion.
They propose a hybrid metric, a linear combination of hop-bytes and three types of congestion statistics. 
The mapping, along with a refinement step, is calculated using a parallelized greedy algorithm.

Faraji et al. \cite{Faraji2016TopoAwareGPUSelection} focus on building a system targeting intranode \gls{GPU} communication.
While their system only works on a single node and relies on SCOTCH's \cite{Pellegrini2012SCOTCH} graph bisecting method to perform the mapping, Mirsadehi et al. \cite{Mirsadeghi2016MAGC} expand the work to an entire cluster.
The complete system uses a 3-step process, first mapping ranks to nodes to minimize network communication, then mapping ranks to cores to optimize intranode communication, and lastly, mappings \gls{GPU}s to ranks for optimal \gls{GPU}-to-\gls{GPU} communications.

These methods excel at optimizing for unique application communication patterns but also tend to neglect collective communications.
The profiling stage is often built on top of the PMPI profiling interface, which can intercept \gls{MPI} calls, but does not break collectives into their constituent point-to-point messages.
Galvez et al. \cite{Galvez2017AutoTopoMap} identify this problem and propose a profiler that groups types of communications into weighted classes.
Their system leverages a parallel algorithm that uses the communication classes, along with a set of weighted metrics, to calculate a near-ideal mapping.
While a step in the right direction, their solution still treats collective communications as a black box, focusing on the collective's communicator and not disassembling the collective into its constituent point-to-point messages.

To decompose collectives into point-to-point messages, Bosilica et al. \cite{Bosilica2017OnlineMonitoringMPI} implemented a monitoring layer in Open MPI that is accessible through the \gls{MPI} Tools interface.
This provides much more granular profiling for point-to-point messages, which allowed Jeannot and Sartori \cite{Jeannot2020ImprvMPICommMonitoring} to propose a rank reordering method for applications with iterative compute, but their solution is required to be implemented within the user's application. 
While it is possible to apply topology awareness to all communication, including collectives, their solution places a significant burden on application developers to implement it within their software.

One of the common shortcomings most of these systems have in common is the required application profiling.
The application must run at least once to build the communication graph, which can be expensive for large-scale runs. 
An ideal solution would be able to build a graph for a moderate amount of processes and project a solution for a large-scale system, but no existing solution can perform this yet. 

The \gls{MPI} standard does provide a few interfaces to allow application developers to inform the runtime of expected communication patterns.  
One of these solutions is virtual topologies. 
This structure is attached to a communicator that tells the implementation how communications are likely to occur between ranks.
Mercer and Jeannot \cite{Mercer2011ImprvMPIWithRR} leverage the \texttt{MPI\_Dist\_graph\_create()} endpoint to reorder processes on a graph topology.
Their solution centralizes the topology information and uses the TreeMatch algorithm \cite{Jeannot2010TreeMatch} to calculate a process reordering.
Gropp \cite{Gropp2019CartTopoMapping} proposes a method for reordering processes in a cartesian topology.
His method relies on \texttt{MPI\_COMM\_TYPE\_SHARED} and neglects network topology as well as memory hierarchy information.

% The most common way to apply topology awareness within collectives is to use a hierarchical strategy.
% This is done by splitting the main communicator into smaller sub-communicators, often one for inter-node resources and one for intra-node resources.
% The intra-node communicator consists of ranks placed on the same node, and each node selects a leader process and the leaders across all nodes for the inter-communicator.
% The collective communication is then decomposed into stages of sub-collectives and  which are executed across the hierarchical structure.
% For example, an allreduce could be implemented in 4 steps: first, an intra-node reduce to the leader, followed by a leader-wise internode reduce to some pre-determined rank, third a leader-wise internode broadcast is performed, and finally, an intranode broadcast to finish the allreduce.
% This method ensures efficient hardware use between shared memory and network resources by placing different stages on different resource layers
There are a plethora of papers that propose hierarchical collective algorithms to provide topology awareness to algorithms targeting specific scenarios.
Lue et al. \cite{Luo2018ADAPT} propose a library for event-based collectives orchestrated through callbacks.
While their work focuses on minimizing synchronization dependencies within collectives, they demonstrate how their work could easily be mapped to a hierarchical algorithm.
Awan et al. \cite{Awan2016NCCLBcast} build a \gls{GPU}-aware hierarchical broadcast algorithm.
Their work leverages the \gls{NCCL} for optimal intra-node communication for large messages and relies on existing algorithms in MVAPICH2 for inter-node communication.
Subramoni et al. \cite{Subramoni2011SpeedAwareBcast} designed a broadcast algorithm that can take InfiniBand topology and network speeds into account.
They gather network information using \gls{OFED} management tools like ibnetdiscover and ibroute \cite{linuxrdmacore} and use the information to build a reordered communicator using either a depth-first traversal or a breath-first traversal.
They only evaluate knomial and scatter-allgather collectives.

Hierarchical algorithms segment the system topology and structure the collective to use appropriate resources accordingly, but the increased complexity has drawbacks.
Decomposing the collective into stages introduces additional implicit barriers and synchronizations, hampering performance. 
While this can be alleviated through pipelining between stages, this adds further complexity and can lead to code maintenance issues.
Further, there are still design challenges with the granularity of sub-communicators.
Stages can be specified for hardware components like \gls{CPU} cache layers, \gls{NUMA} domains, or different layers of the network topology, allowing for more optimal hardware usage, but more stages add more memory requirements (for more sub-communicators) and more implicit synchronizations between stages.
Also, introducing multiple stages can create additional data transfers compared to a flat algorithm.
Large message algorithms like ring and \gls{RSA}, are designed to transfer as little data as possible, however, hierarchical algorithms are composed of multiple flat algorithms, so it is virtually impossible for a hierarchical algorithm to send less data than a flat ring/\gls{RSA}.
Our method mitigates many of these issues as it does not rely on multiple sub-communicators or algorithm stages to complete the operation.

The work presented in this thesis is an extension of work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR}.
Instead of building a communication graph to custom fit the host topology, this work leverages the existing algorithm but renumbers the ranks behind the scenes to better map the communication graph to the hardware.
Their work focuses on allgather collectives, with remapping algorithms targeting ring, recursive doubling and binomial communication patterns.
We extend their work by looking at allreduce and the additional challenges the reduction operation adds to their method.
Some of their proposed heuristics are applicable to allreduce algorithms, so our work extends some of their proposed heuristics to an allreduce context.
We also propose new mapping heuristics for \gls{RSA}, knomial and binary-tree communication patterns.
Furthermore, we evaluate collective reordering in on modern hardware, including large-scale evaluations on \gls{GPU} clusters.


\section{Design Methodology}
When applying topology awareness to collectives, there are two common strategies: 1) either build a communication pattern that is designed to fit the hardware (this is what hierarchical algorithms do); or, 2) take an existing pattern and efficiently map it to the hardware.
This work uses the latter strategy, we take the collective algorithm selected by the implementation and silently renumber the processes to utilize communication resources better.
\gls{MPI} providers will often include multiple algorithms for different collectives with common patterns, and each algorithm has a communication pattern implicit in the algorithm's design.
For example, the ring algorithm only exchanges data with its immediate neighbours, and the recursive doubling algorithms only exchange data with ranks that are a power of two away.
Since these patterns are pre-defined, we can extract a communication pattern and combine it with topology information before the collective is run.
This problem can be formalized as an instance of a graph-embedding problem, which previous work has shown to be an NP-Hard problem \cite{Hoefler2011GenericTopoMappingStrats}. 
So solving this problem at scale (on the order of millions of processes) is not feasible. 
Therefore, heuristics and simplifications are often used instead.

Since we are interested in accelerating distributed deep learning, we target several allreduce algorithms, as well as a few broadcast algorithms, since they can be combined with a reduce+broadcast and are occasionally used to implement allreduce.
Each proposed remapping heuristic leverages a greedy strategy to make the calculation efficient and scalable, and the general process is outlined in Algorithm \ref{alg:toporr-strat}.
The basic structure starts by binding rank zero to core zero and sets zero as the reference rank.
In the heuristics main loop, Lines 3 to 8, all process remappings will try to bind to resources as close to the reference rank as possible.
While the heuristic loops through the ranks, it selects the next rank to map based on the communication pattern (Line 4) and updates the reference rank when required (Line 7).

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/TopoRR_strat_pseudocode}

The mapping is enforced by creating a new communicator.
Each process calls \texttt{MPI\_Comm\_split()}, requesting their new rank, and then the selected algorithm is run on the resulting communicator.
Since communicator creation is an expensive operation, this shadow communicator is cached so that it can be used again if the original communicator uses the same algorithm.

This structure is flexible and allows us to evaluate multiple types of collectives.
Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} propose heuristics targeting ring allgather, recursive doubling allgather, and binomial tree broadcast. 
We reimplement their ring and recursive doubling algorithms, evaluate them on allreduce collectives, and evaluate their binomial-broadcast heuristic in the context of a scatter-allgather broadcast.
Then, targeting large message allreduce, we propose a new reordering heuristic for reduce-scatter-allgather allreduce, along with heuristics for two broadcast algorithms, knomial and binary tree broadcast.


\subsection{Reduce-Scatter-Allgather Allreduce}
\gls{RSA} allreduce, also known as Rabenseifner's algorithm, is a popular algorithm implemented in most \gls{MPI} implementations and even used in modern collective libraries like UCC \cite{Rabenseifner2004OptOfCollRedOps}.
The algorithm starts with each process performing a recursive-vector halving reduce-scatter, this distributes a segment of the fully reduced vector to each rank.
At this point, each rank needs to gather the other reduced segments from the other ranks, which is achieved through a recursive doubling allgather.
\gls{RSA} is used for large message allreduce because it efficiently uses system bandwidth.
For an allreduce with $p$ processes on $n$ bytes of data, each rank sends $2((p-1)/p)n\beta$ bytes of data, linear scaling with message size is ideal for large message collectives.
In terms of structure, since both phases rely on a recursive pattern, data is only exchanged with ranks that are a power of 2 distance away.
The other important feature of \gls{RSA}, is that more data is exchanged with numerically closer ranks.
Figure \ref{fig:graph-rsa} demonstrates this phenomenon with eight processes.
Most data is exchanged with immediate neighbours, this accounts for the first stage of reduce-scatter and the last stage of the allgather.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/Topo_graph_rsa}

So while designing our heuristic, we want to map communicating pairs that are a power of two distances away, focusing on small distances first.
With these goals in mind, we propose the heuristic in Algorithm \ref{alg:rsa} to target the \gls{RSA}, algorithm.
This heuristic starts by mapping rank 0 (Line 1), then walks to the power of 2 communication graph mapping processes that have not been mapped yet (Lines 5-10). 
While looping through all the ranks, the reference rank is updated every two mappings (Lines 11-14), this ensures that communicating pairs that are distance one away are mapped as close as possible. 

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/rsa_pseudocode}

\subsection{Binary Tree Broadcast}
Binary trees benefit from their simplicity.
Ranks receive one message and forward it to two child ranks.
The tree structure provides logarithmic scaling to the number of messages sent, so binary trees are often used for smaller message sizes. 
A typical structure for a binary tree is provided in Figure \ref{fig:graph-bin-tree}.
For a rank $r$, the height in the tree can be calculated as $h_r = \lfloor log_2(r+1) \rfloor$, and ranks will send messages to $r + 2^{h_r}$ and $r + 2^{h_r + 1}$
This leads to a structure where processes further down the tree send messages to ranks further and further away.
Furthermore, binary trees are constructed slightly lopsided, as nodes are added to the left side of the tree before the right side. 
This can lead to situations where one direction has slightly more nodes than the other, which should be accounted for when designing the heuristic.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/Topo_graph_bintree}

To ensure that ranks reside on the same node as their children, we propose a depth-first-traversal-based heuristic with the code provided in Algorithm \ref{alg:bintree}.
We employ a recursive function to traverse the tree efficiently. 
We start by mapping the tree's root (Line 1) and passing it in as the reference rank to the recursive function (Line 2).
The recursive function starts by calculating the reference rank's height and child processes (Lines 4-7). 
Then, depending on if the child processes are in the communicator, we bind them close to the reference rank and recuse with the new process set as the reference rank (Lines 8-15).
To account for lopsidedness, the heuristics traverses the potentially smaller side first by mapping the numerically greater child first.
We found that this does a more consistent job in mapping the tree to the system architectures we evaluated.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/bin_tree_pseudocode}

\subsection{Knomial Broadcast}
Knomial is a generalization of a binomial tree, and its structure can be defined inductively.
For a knomial tree $T_{k,h}$ of height $h$ and radix $k$, if $h=0$, then $T_{k,h}$ is a single node. For $h>0$, $T_{k,h}$ is built from $k$ copies of $T_{k,h-1}$, where the root of the first copy is the parent node of the other $k-1$ copies.
Figure \ref{fig:graph-knomial} provides an example of a knomial tree with radix 4 and 16 processes.
While any value of $k$ can be used, implementations commonly use either two (binomial tree) or four as default values. 
While the tree's structure is heavily lopsided, in theory, this provides the opportunity to overlap message transfers at lower layers of the tree with transfers higher in higher layers.
Due to the inductive definitions, high $k$ value trees will have a lot of leaf nodes, as each subtree will contain $k-1$ subtrees with $h=0$.

In Algorithm \ref{alg:knomial}, we propose a depth-first traversal heuristic, which focuses on mapping smaller subtrees first.
By focusing on subtrees, we are trying to group bunches of leaf nodes on the same node as their parent.
Since this is a depth-first traversal, we use another recursive function to traverse the tree. 
Once again, we start by mapping the tree's root (Line 1) and passing it in as the reference rank of the recursive function (Line 2).
In the recursive function, we immediately map the $k-1$ subtrees of height 0 (Lines 4-6).
Then we use a while loop to calculate the values for subtrees that are further away (Lines 8-19).
If the root of a subtree does exist in the communicator, we map it as close as possible to the reference rank (Line 14), calculate its depth (Line 15), and recurse on it as the reference rank (Line 16).

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/Topo_graph_knomial}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/knomial_pseudocode}

\subsection{Ring Allreduce, Recusive Doubling Allreduce, and Scatter-Allgather Broadcast}

Previous work by Mirsadeghi and Afsahi \cite{Mirsadeghi2016TopoAwareCollRR} proposed a series of rank reordering heuristics targeting recursive doubling allgather, ring, binomial broadcast, and binomial gather. 
Their work is evaluated in the context of allgather, but the heuristics they propose can also be applied to other algorithms.
We identified three algorithms that can be efficiently remapped using existing heuristics: ring allreduce, recursive doubling allreudce, and scatter-allgather broadcast.

The ring and recursive doubling allgather heuristics can be applied directly to ring and recursive doubling allreduce, respectively. 
The intuition behind both heuristics' utility holds since there is not much difference between the allgather and allreduce versions of the same algorithm.
The ring heuristic targets nearest neighbour ranks, it starts at rank 0 and walks through the communicator mapping $r+1$ as close as possible to reference rank $r$, then setting $r+1$ as the new reference rank.
Since both ring allreduce and ring allgather only communicate with immediate neighbours (i.e., rank $r+1$ and $r-1$), the intuition of this heuristic is portable across collectives.

The recursive doubling heuristic targets the last two stages of the algorithm, as message sizes are the largest during those phases (of allgather). 
It is similar to our proposed \gls{RSA} heuristic but focuses on the last rounds of communication instead of the first. 
Recursive doubling allreduce and allgather share the same communication pattern, but while the message sizes double each round in allgather, it remains fixed in allreduce.
So migrating the heuristic to allreduce might not see the same performance gains, but it will ensure that most communications take an efficient path.

We also evaluated the previously proposed recursive doubling allgather heuristic in the context of the scatter-allgather broadcast.
Scatter-allgather, also known as Van de Geijn's algorithm, is often used for large message broadcasts as it is incredibly bandwidth efficient. 
This is achieved by splitting the message into segments, scattering the segments across the communicator with a binomial algorithm, and then reconstructing the message with a recursive doubling allgather allgather.
The recursive doubling allgather has already been studied in \cite{Mirsadeghi2016TopoAwareCollRR}, it has an even communication pattern where the last stages of the algorithm have the largest message sizes.
On the other hand, the binomial scatter is similar to the binomial broadcast algorithm, with the difference that the message size halves between stages. 
The communication graph generated by the binomial pattern is essentially a subgraph of recursive doubling, so it can be layered on top of recursive doubling without adding any new communication dependencies. 
Therefore, we applied the recursive doubling reordering heuristic to scatter allgather broadcasts.

\section{Experimental Evaluation and Analysis}
We evaluated our new heuristics and the existing heuristics on \gls{CPU} and \gls{GPU} allocations across two different clusters.
To ensure the robustness of our algorithm, we evaluate different initial mapping to stress test other starting conditions.
Furthermore, to evaluate against more traditional topology mapping methods, we compare our heuristics against SCOTCH \cite{Pellegrini2012SCOTCH}, a commonly used graph partitioning library previously applied to topology-aware works.

\subsection{Experimental Platform}\label{sec:CH4-experimental-platform}
We evaluated our work on two heterogeneous clusters, Beluga and Cedar, both provided by Compute Canada. 
Beluga is InfiniBand based, structured as a 5:1 blocking fat tree with static routing.  
Cedar, on the other hand, is OmniPath based, while it also has a fat-tree topology, it is configured with 2:1 blocking and adaptive routing.

Beluga's \gls{CPU} nodes have dual socket Xeon Gold 6148 Skylake processors, and the \gls{GPU} nodes use the same \gls{CPU}s attached to four V100s fully connected with NVLink.
Cedar's \gls{CPU} allocation has dual-socket 24-core Xeon Platinum 8260 Cascade Lake processors, while the \gls{GPU} nodes have two 20-core Xeon Silver 4216 Cascade Lake processors hosting four v100s \gls{GPU}s fully connected by NVLink.
The critical difference between Beluga and Cedar's \gls{GPU} nodes is the \gls{PCIe} connections between \gls{CPU}s and \gls{GPU}s.
On Beluga, all the \gls{GPU}s are connected to socket 0 through a \gls{PCIe} switch, while Cedar has \gls{GPU}s split between sockets 0 and 1, each directly connected to the socket.

Both clusters are Linux-based, running CentOS-7. 
All software was compiled using GCC v9.3.0, and all experiments were based on Open MPI v4.0.5 built with \gls{UCX} v1.8.0.
When evaluating \gls{GPU} collectives, the \gls{CUDA} v11.0 toolchain was used.

\subsection{Implementation Details}\label{sec:CH4-impl-details}
We implemented our method by modifying Open MPI v4.0.5 \cite{gabriel2004OpenMPI}.
Building on top of Open MPI lets us leverage the existing collective implementations as well as the algorithm selection mechanism.
Open MPI also has internal support for Hwloc \cite{Broquedis2010hwloc}, which makes gathering topology information easier.

There are some caveats with collective rank reordering, as not all collectives can support it seamlessly.
By default, \texttt{MPI\_Allreduce()} algorithms assume operations are commutative with respect to the ranks, but users can specify operations as non-commutative.
To support non-commutative operations, allreduce algorithms must be carefully designed to ensure operations are applied in the correct order, but renumbering processes break that careful design.
Therefore, this work only supports commutative operations.
Furthermore, both broadcast reordering heuristics assume rank 0 is the root which is not always true. 
So when rank 0 is not the root, it needs to receive the data from the real root before it can start the broadcast on the reordered communicator.
% (In retrospect, a more intelligent design would have been to assign virtual ranks by shifting each rank according to the root's value, this would make the root 0, but I last looked at the code for remapping a year ago, and the performance difference provided by this step would be negligible; furthermore, OMB only calls broadcast with root 0, and Horovod rarely uses broadcast.)

We leverage existing system topology detection tools to gather topology information for the node-local topology and the network topology.
Hardware info was gathered through Hwloc \cite{Broquedis2010hwloc}, a tool that repackages information provided by the operating system and presents it in a user-friendly way.
Hwloc is a runtime library that can efficiently provide information on \gls{NUMA} domains and cache hierarchies during job execution.
Thankfully, Hwloc is system agnostic as it mainly relies on endpoints standardized by the operating system, but conversely, network monitoring/management tools are vendor specific.
To extract the network topology, we leveraged \texttt{ibnetdiscover} for InfiniBand and \texttt{opareport} for OmniPath, both tools scan the entire network and require admin privileges to run, so integrating them at runtime is not a viable option.
Both tools generate text files describing how network ports are connected, along with other info like speed and link status.
Since the tools have different outputs, they were transformed into an intermediate format and saved to disk.
Topology information is represented as a matrix at runtime, and each node is responsible for generating the row corresponding to its rank.
Within each row, values represent the cost of sending data to other ranks, and an allgather is used to combine all processes' node information and generate the final results.
(This method is not that scalable as the memory requirements are on the order of $O(n^2)$ with respect to the number of processes, but it is viable for the scale we are evaluating.)

Open MPI does support allreduce with \gls{CUDA} memory, but their implementation is pretty naive.
When allreduce is called on \gls{GPU} memory, the data is copied to a temporary host buffer, then a \gls{CPU}-based allreudce is performed, and the final result is copied back to the \gls{GPU}.
While this does fulfill the \gls{MPI} specification, it neglects to use the available \gls{GPU} hardware like compute kernel reduction or \gls{GPU} to \gls{GPU} data transfers with NVLinks.
This implementation has benefits in terms of code maintenance (introducing \gls{CUDA} kernels would be a nightmare to manage in terms of compiler toolchains), but the performance is abysmal. 
So we modified Open MPI's allreduce algorithms to use \gls{GPU} hardware, this technique is more in line with how modern collective libraries like \gls{NCCL} and \gls{UCC} implement allreduce \cite{UCC, NCCL}.

The rank reordering strategy finds performance improvements by rearranging the process to core bindings, implying that any performance improvements depend on the initial process to core bindings.
\gls{MPI} implementations provide flags for users to specify how processes should be distributed at job launch. 
While not exhaustive, this interface does provide a starting point for users to start optimizing process to core mappings.
However, our method can calculate more complicated mappings and apply mappings to subsets of \texttt{MPI\_COMM\_WORLS} on a per-communicator level.

In Open MPI, users can specify a resource and processes are bound to that resource in a round-robin fashion.
Figure \ref{fig:init-mappings} provides an example outlining the three initial mappings we evaluated.
The default mapping is by-package, this takes a node and fills it with processes alternating between sockets until the node is full, it then repeats this on the next node until all processes are bound.
By-core mapping places processes to consecutively numbered cores filling up a node before moving to the next, and by-node scatters processes across nodes by cycling through nodes and placing processes one at a time.

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/init_mapping}

To compare our proposed heuristics against an established topology mapping tool, we also integrated the SCOTCH graph embedding tool \cite{Pellegrini2012SCOTCH} into the rank reordering method.
SCOTCH uses a graph bisecting method to perform graph partitioning and embedding, and it has been used in previous work to incorporate topology awareness \cite{Mirsadeghi2016TopoAwareCollRR, Bordage2018Netloc, Deveci2015FastHQTopoAwareTaskMapping}.
Out of the box, Open MPI relies on a static tuning table to choose a collective algorithm based on the communicator size and message size, so we applied SCOTCH to whichever algorithm the tuning table selected.
Due to technical reasons, we could not get SCOTCH to work on Cedar, therefore we only present SCOTCH results on Beluga.

\subsection{Performance Results and Analysis}

\subsubsection{Micro-benchmark Results}

We used OSU Micro-Benchamrks v5.7 \cite{Bureddy2012OMB} to evaluate the performance of our proposed method. 
The micro-benchmark loops over \texttt{MPI\_Allreduce()} measuring the runtime of each function call and outputs the average latency. 
It also provides the option to locate the data buffer in either \gls{CPU} or \gls{GPU} memory.
GPU allocations were evaluated with four processes per node (since there are only 4 \gls{GPU}s per node), and \gls{CPU} allocations were evaluated with 32 processes per node because that is the largest power of two that can fit on a node without oversubscribing resources.
We evaluated large jobs on Beluga using 128 ndoes at a time, giving 512 processes for \gls{GPU} jobs and 4096 ranks for \gls{CPU} jobs.
Experiments on Cedar were a bit smaller, with \gls{GPU} allocations of 32 nodes (communicator size of 128) and \gls{CPU} allocations with 64 nodes (communicator size of 2048).

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/ar_beluga_128}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/ar_cedar_32g_64c}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/bc_beluga_128}
\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/bc_cedar_32g_64c}

The results for allreduce are similar across both clusters, with results on Beluga given in Figure \ref{fig:beluga-ar-128}, and results on Cedar in Figure \ref{fig:cedar-ar-32g64c}.
While results on Cedar are not identical to Beluga due to differences in computer architecture and the number of processes, the benefits of our proposed methods are consistent across both clusters.
Across all evaluated examples, at least one of the remapped algorithms performs better, or on par, with the default mapping. 
There would be an opportunity to restructure Open MPI's tuning table to select the most optimal remapped algorithm, providing a more performant \texttt{MPI\_Allreduce()}.
There are scenarios where SCOTCH can improve performance, like small message \gls{GPU} allreduce, but it is most often the case that it negatively impacts performance.

For allreduce, the most impactful results are \gls{CPU}s remapped from by-node.
Beluga can see a 60\% improvement over the default for \gls{RSA}, and as big as 74\% at certain sizes like 512kB.
Cedar's \gls{CPU} allreduce has an even more significant improvement with 85\% for 64MB.
The \gls{GPU} allreduce on Beluga does not see that much improvement, but the results on Cedar are substantial, with 64MB ring seeing 68\% improvement, 64MB \gls{RSA}, seeing 54\% improvement, and \gls{RSA}, seeing 90\% improvement at 8MB and 16MB.

The broadcast results are a bit more varied, with Beluga's brodcast data given in Figure \ref{fig:beluga-bc-128}, and Cedar in Figure \ref{fig:cedar-bc-32g64c}.
For \gls{GPU} resources, the best results are seen on a by-node mapping.
The by-node results on Beluga's \gls{GPU}s ran into message size scaling issues, but the largest size we could collect (8MB) had a 70\% improvement over the default, and while not as extreme, Cedar still sees 42\% improvement for 64Mb on a by-node mapping.
Interestingly, the \gls{CPU} results tend to prefer the by-socket and by-core mappings. 
The knomial and binary tree results both see 80\% performance improvements for all large messages on Beluga, and the binary tree remapping sees a 51\% improvement on the same initial mappings on Cedar.

The benefits of remapping are often most prevalent on a by-node initial mapping. 
For example, in Figure \ref{fig:cedar-ar-32g64c}, ring allreduce on \gls{GPU}s can beat the default by 3\%-7\% for large messages on by-socket and by-core mappings but sees a 30\%-60\% improvement for the same message range on a by-node mapping.
Open MPI's collective algorithms tend to be structured so that processes exchange most of their data with nearby ranks, this design characteristic is most pronounced in algorithms like \gls{RSA}, ring, and knomial.
This design decision tends to perform well on by-socket and by-core mappings, which reflects our results where default Open MPI performs similarly to many of our remapped algorithms on by-core and by-socket initial mappings.
But, the same algorithms tend to struggle with by-node initial mappings because each process's nearby ranks are off-node, which leads to a lot more data traversing the network.
The binary-tree mapping is an exception to this pattern as child ranks are often much further away than parent ranks, therefore, binary tree remapping has much more potential than other algorithms.
The topology awareness of our remapping method solves this issue. 
It relieves network pressure by moving most of the communication back within the node and tends to outperform the default by a large margin.

The impact of remapping is much more noticeable on \gls{CPU} resources than on \gls{GPU}s.
For example, in Figure \ref{fig:beluga-ar-128}, reordering large message \gls{RSA}, on \gls{CPU}s mapped by-node sees around 60\% improvement over the default, while the same algorithm only sees 15\% on \gls{GPU}s.
This can be justified through topology awareness' ability to relieve contention on resources like network cards or inter-socket links. 
Both the \gls{CPU} and \gls{GPU} nodes have a single network card, but the \gls{CPU} benchmarks have 8x more processes per node than the \gls{GPU} jobs, creating a lot more demand for the network card on each node.
Further, \gls{CPU} jobs have to manage contention for the inter-socket link, the \gls{GPU} architecture avoids this issue since all the cards are fully connected.
While both allocations benefit from remapping, the results are more impactful for the \gls{CPU} jobs as they can redirect much more of the resource demand.

\subsubsection{Scalability}

\input{3_Chapters/4_Chapter_TopologyAwareness/Figs/log_plots/beluga_scale}

The remapping method does retain its performance wins across communicator sizes. 
Figure \ref{fig:beluga-scale-64m} demonstrates how each algorithm compares scaling a 64MB collective from 32 nodes to 128 nodes.
Only a by-socket mapping is presented, but by-core and by-node have similar scaling characteristics.
Some of the heuristics are still hampered by their underlying algorithm, recursive doubling being the worst offender, but the performance improvements scale well across the board.

\subsubsection{Applications}
We also evaluated the effects of remapping allreduce on data-parallel \gls{DL} training, but there was no noticeable impact.
We tested a series of models using Horovod's synthetic benchmarks across \gls{CPU} and \gls{GPU} resources on Beluga and Cedar, we also evaluated the same set of initial mappings. 
Still, there was no discernible difference in performance between remapped and non-remapped collectives.

There are many possible reasons why the micro-benchmark performance improvements do not translate to the application.
One possibility is that the percent improvement in micro-benchmark performance is not significant enough to impact Horovod's performance.
Allreduce micro-benchmarks on Beluga saw a negligible impact in allreduce improvement for the message sizes, so no improvements there are understandable.
Even though ring and \gls{RSA} can see a 68\% improvement on Cedar, sadly, there are no performance measurable improvements observed in Horovod.

There is also the possibility that any performance gains are hidden by compute-communication overlap.
Horovod is designed so that the \gls{GPU} communications happen on a background thread concurrently with the \gls{GPU} computations. 
During the backwards pass of the \gls{SGD} algorithm, Horovod effectively pipelines the large message \texttt{MPI\_Allreduce} and weight update steps to hide latency at the application level.
This means that performance gains could be lost due to time spent performing weight updates or managing the pipeline. 

There is also the challenge that the allreduce message sizes that see the strongest improvements are not being used by Horovod.
Horovod tends to send large messages, specifically 64MB, the default setting of the \texttt{TENSOR\_FUSION\_THRESHOLD}.
However, the strongest results, like the 90\% improvement for ring and \gls{RSA} with 8MB and 16MB on Cedar, are most likely not being used, so they are not reflected in application performance.

The \gls{CPU} benchmarks might not be as performance-bound by communication as \gls{GPU} benchmarks.
Horovod has been shown to have good strong scaling properties on \gls{CPU} clusters \cite{Jain2019ScaleTFPTMXNonFrontera}.
Since \gls{CPU}s are much slower than \gls{GPU}s, it is likely that a larger fraction of the benchmark's runtime is spent in computation.
So while \gls{CPU} allreduce remapped from by-node outperforms default Open MPI by 85\%, the performance improvements are not reflected in Horovod. 

\section{Conclusion}
This chapter adapts topology-aware collective rank reordering to allreduce and new broadcast algorithms.
We show that this method can be applied to \gls{RSA}, ring, and recursive doubling allreduce, along with binary-tree, knomial, and scatter-allgather broadcast.
It works across different initial mappings but sees the best results on a by-node mapping.
While this work is best applied to dense \gls{CPU} allocations, it can also be applied to \gls{GPU} allocations. 
While not explicitly demonstrated, at least one of our proposed remappings consistently performs on par or better than default Open MPI, so we could theoretically build a tuning table to provide better overall \texttt{MPI\_Allreduce()} performance. 

The idea of this work was to accelerate allreduce through the topology-aware algorithms, with the motivation of decreasing deep learning training time.
Topology awareness has been exhaustively evaluated by many researchers, with a plethora of ideas existing in the literature.
But, topology awareness is not the only strategy for accelerating allreduce.
In the next chapter, we tackle the same problem of accelerating allreduce, but from the angle of PAP awareness.
This is still in the domain of large-message allreudce, so many existing algorithms are applicable. 
However, \gls{PAP} has numerous challenges requiring clever and innovative ideas to solve.
