% Chapter 5 - Process Arrival Pattern Awareness

\glsresetall % reset the glossary to expand acronyms again
\chapter[PAPAwareness]{Process Arrival Pattern Awareness}\label{ch:PAPAwareness}
\index{Process Arrival Pattern Awareness}

\begin{itemize}
    \item Background on Process Arrival Pattern Awareness
    \begin{itemize}
        \item Pedram's characterization work, mention that Horovod can see PAP imbalacen in range of 2-6, \cite{Alizadeh2022PAPCollDL, Mohammadalizadehbakhtevari2021Thesis}
        \item Maximum/Average Imbalance Factor and EQs
        \begin{itemize}
            \item Identify synchronization/data dependencies, propose a solution to synchronization dependencies using callbacks \cite{Luo2018ADAPT}
        \end{itemize}
    \end{itemize}
    \item Methods
    \item Evaluation
    \begin{itemize}
        \item Horovod? Cosmoflow?
    \end{itemize}
\end{itemize}

\section{Motivation}

This chapter approaches allreduce collective design through the lens of process arrival pattern (PAP) awareness. 
When designing collective algorithms, developers often assume that all processes arrive at the same time, but this is not a safe assumption.
The order and timing that processes enter the collective can have an impact on the algorithm's performance, and this work tries to leverage the arrival imbalance to improve the overall collective performance.

There are previously proposed methods and algorithms that leverage PAP awareness to accelerate collectives, this chapter starts by surveying existing work.
We outline some analysis on how PAP affects collectives, outlining our approach to accelerate allreduce.
The main contribution is a cluster-wide method of sharing PAP information with an accommpanying allreduce algorithm designed to perform better than existing algorithms under a processes arrival imbalance.
We evaluate our work with using an imbalance factor-inducing benchmark and show that we can see \_\% improvement over state-of-the-art algorithms.  

\section{Background}

\subsection{Process arrival imbalance} % move to background, in PAP subsection?
As defined in \cite{Faraj2008StudyProcArrivalMPIColl}, for processes $(p_0, p_1,...,p_n)$ participating in a collective operation, each process has an arrival time denoted by $a_i$ and an exit time at $f_n$.
Therefore, each collective call has an arrival patterns $(a_0, a_1, ..., a_n)$ and an exit pattern $(f_0, f_1, ..., f_n)$.
Each node spends $e_i = f_i - a_i$ time in the collective, where total collective time can be expresses as $e_0 + e_1 + ... + e_n$ and the average collective time is $\bar{e} = (e_0 + e_1 + ... + e_n)/n$, we can use these two terms to evaluate the performance of the overall collective.
In order to quantify the difference between processs arrivals, let $\delta_i$ be the absolute time difference between a processes arrival and the average arrival time, i.e. $\delta_i = |\bar{a} - a_i|$. 
We can then define the average arrival imbalance as $\bar{\delta}=(\delta_0 + \delta_i + ... + \delta_n)/n$, and the maximum imballance time as $\omega = max_i(a_i)-min_i(a_i)$.
When using average/max imbalance in practice, it is often easier to normalize it by the message transmision time.
If the time to send a messages of $n$ bytes is $T$, then the \textit{maximum imbalance factor} is of that message is $\omega/T$, and the \textit{average imbalance factor} is $\bar{\delta}/T$.


\subsection{Synchronization Mechanism} % move to background, in PAP subsection?
One of the predominant issues with PAP-Awareness is actually determining the order of process arrival.
Each process needs some level of arrival information to determine the steps it needs to perform within the collective algorithm. 
One strategy for inter-node PAP-awareness can include exchanging control messages using MPI point-to-point routines leveraging MPI\_Any\_Source \cite{Patarasuk2008EffBcastDifProcArr}.
These strategies aren't ideal, as these mechanisms require the active participation of some leader processes.
This is often delegated to the root in one-to-many or many-to-one collectives but doesn't map as easily to many-to-many collectives.
Further, this adds overhead and worsens workload imbalance as one process spends considerably more cycles orchestrating process arrival information.
There is also the issue of processes stalling before the root has arrived, as no work can be done before PAP information is exchanged.
Some proposed work fixes this by delegating PAP management to a background thread \cite{Proficz2018ImprvAllReduceForImbPAP, Proficz2020PAPAwareScatterGather, Proficz2021AllGatherResilientToImbPAP, Faraj2008StudyProcArrivalMPIColl}, but this solution can add more imbalance due to thread scheduling, as well as a burden on application developers to manage resources of background threads.

Other internode solutions rely on non-portable interfaces like RDMA on Infiniband \cite{Qian2009ProcArrivalSHMA2AIB}.
This does lock the solution to specific hardware, but it is promising as PAP information can be exchanged in a one-sided manner.
Since processes don't synchronize, they can exchange PAP information before other processes have arrived, and without some central process managing the entire collective. 
To provide more portability, we want to leverage a higher-level programming interface, and our first choice would be MPI's one-sided communications, but the problem with MPI is that the synchronization model is too burdensome. 
In order to access a target process' memory, the origin processes need to synchronize through an access epoch. 
Active target communication is out of the question as one of our goals is to minimize the amount of synchronization, and MPI\_Win\_Fence is a collective, so it also defeats the purpose, this leaves  MPI\_Win\_lock/MPI\_Win\_unlock as the only viable synchronization mechanism.
On the other hand, we could try to ignore synchronization by starting an access epoch with MPI\_Win\_Fence at MPI\_Init, and imposing order through MPI\_Fetch\_and\_op, but MPI's memory model doesn't guarantee that public/private windows will be updated unless synchronization occurs, so that means we still need MPI\_Win\_lock/MPI\_Win\_unlock synchronization.

Alternatively, there are deeper layers in the MPI stack. 
Under the hood, MPI implementations will target a transport-focused layer to drive the network hardware, this design choice is done for portability purposes as the transport layer can support multiple types of hardware, and different programming models can target the transport layer APIs.
The two most used layers are Libfabric and UCX \cite{libfabric, shamis2015ucx}.
These APIs are designed to provide a wrapper around network resources, with a balance between portability and performance.
Since they are designed to emulate hardware, they do not have the same syncronization challenges that MPI imposed on users.  
Neither library has the notion of public/private memory, and the completion of one-sided operations can be tracked on a per-operation basis. 
Operations in Libfabric relies on completion queues.
Communication operations are associated with a completoin queue, and applicatoins poll the queueu to check 

\subsection{Non-PAP-Aware Algorithms} % more appropriate in collectives subseciton of backgroundor or methods subsection
This work is targeting large message allreduce with the hope of accelerating model paralle distributed DL training.
When performing a large messages allreduce, the most optimal algorithms are either ring or reduce-scatter-allgather.
These are the algorithms most often used in practice, with Open MPI, MPICH, and UCC chosing to use reudce-scatter-allgather, and NCCL leveraging a ring-based strategy optimized for their hardware topology \cite{gabriel2004OpenMPI, MPICH, UCC, NCCL}.
The dominance of these models can be easily explained thorugh modeling used by Thakur et al. \cite{Thakur2005OptMPICH}.
They leverage Hockney's model \cite{Hockney1994HockenyModel} where a send operation of $n$ bytes can be represented as $\alpha+n\beta$.
$\alpha$ represents the overhead cost (seconds) and $\beta$ is inverse bandwidth (seconds per byte).
They also add a $\gamma$ term which represents the time to perform a reduction in seconds per flop. 
When dealing with large data tranfers, we tend to see $n\beta>>\alpha$ and $n\gamma>>\alpha$ 

For a communicator of $p$ processes, the cost of performing a ring allreduce is $T_{ring} = 2(p-1)\alpha + 2((p-1)/p)n\beta + ((p-1)/p)n\gamma$, and the cost of performing RSA is $T_{rsa} = 2\log(p)\alpha + 2((p-1)/p)n\beta + ((p-1)/p)n\gamma$.
The common factor between both models is how the $\beta$ and $\gamma$ terms scale linearly w.r.t message size, and sublinearly w.r.t communicator size.
This is acheived by dividing the reduction vector into segments and distributing partial reductions of all segments accross all the ranks.
This is explicitly done with the reduce-scatter phase of the RSA algorithm, but is also implicitly done during the first half of ring allreduce.
When performing one of these algorithms on a communicator, each processes in the communicator needs to send the same amount of data.
This is useful for load balancing if you assume all processes arrive at the same time, but when processes arrive out of order the entire communicator is delayed by the time it takes for the last process to performe the entire collective.
In other words, for large message allreduce, the time of the last process can be described as $T_{last\_proc}=2((p-1)/p)n\beta + ((p-1)/p)n\gamma$.

\subsection{Related Work}
Mamidala et al. \cite{Mamidala2004BarrierAllreduceIBAdaptive} propose a PAP-Aware tree-based algorithm that can be applied to barrier and allreduce collectives.
Their method involves rebalancing a k-ary tree by passing a token between ranks.
For the process holding the token, when $k-1$ child processes have arrived, the token is passed to the $k^{th}$ child through an RDMA write.
If an arriving process is holding the token and all its child processes have arrived, it knows it's the last to arrive, so it triggers a multicast releasing all ranks from the barrier. 

Faraj et al. \cite{Faraj2008StudyProcArrivalMPIColl} evaluate process arrival imbalance at collectives across a handful of MPI kernels.
The PMPI profiling interface was used to introspect the kernel runtime and collect PAP statistics on two different clusters.
The authors determine that process imbalance is unavoidable; even if the workload is perfectly balanced at the application level, the complexity of these massive systems will inevitably lead to differences in arrival time. 
But, regular imbalance patterns do emerge during application runtime, and specific collective call sites will exhibit the same imbalance multiple times during execution.
They also measure the effect of process imbalance on specific algorithms for broadcast and alltoall outlining how process imbalance can be an important factor in algorithm selection.
The authors propose a method for PAP-Aware dynamic algorithm selection based on STAR-MPI \cite{Faraj2006StarMPI}.
This method monitors collective execution at the granularity of each call site, and selects an optimal algorithm based on observed PAP Imbalance. 

Patarsuk and Yuan \cite{Patarasuk2008EffBcastDifProcArr} investigate the impacts of PAP on broadcast algorithms.
Through modelling, they show how all existing algorithms can suffer from substantial performance loss to process imbalance. 
The authors propose a new broadcast algorithm that dynamically assembles sub-groups to perform the broadcast operation.

Qian and Afsahi \cite{Qian2009ProcArrivalSHMA2AIB} propose a method for applying PAP-Awareness to alltoall in Infiniband Clusters.
They modify a direct alltoall algorithm so that data exchanges are not ordered.
The algorithm is built on top of InfiniBand's RDMA semantics, which means processes need to share destination addresses that peers can write data into. 
Their method relies on using the RDMA address as a notification mechanism alerting processes of when their peers have arrived and where to write relevant data.
They also extend this idea to a hierarchical type algorithm, allowing them to leverage shared memory for intranode transfers.

Parsons and Pai \cite{Parsons2015ExpProcImbMPICollHierarcialSys} study process imbalance on their Cray XE6 in a similar way to Faraj et al. \cite{Faraj2008StudyProcArrivalMPIColl}.
They go a bit more in-depth by investigating performance counters using PAPI \cite{Mucci1999PAPI}, but they arrive at the same conclusion that the system is too complex and that none of the observable counters strongly correlate with processing imbalance. 
The authors propose a dynamic leader selection method to build hierarchical pap-aware algorithms for reduce and broadcast.
They use a shared memory structure for intranode communications, where the last/first processes to arrive is selected as the leader for the reduce/broadcast algorithms, respectively. 
Since any rank could be dynamically selected as a leader, parent/child relationships for the internode binomial tree are established using control messaged with MPI\_ANY\_SOURCE. 
They also propose an alltoall algorithm, but they found that the overhead of the control messages is too great, so instead they impose a static multileader hierarchical pattern.
In order to take advantage of arrival patterns, the authors propose \textit{opportunistic message fragmentation}, criteria that leaders can use to select chunks of data to send before all processes have arrived.
This work only applies PAP awareness at an intra-node level; while there is a leader identification mechanism to set up inter-node exchanges, it does not leverage any PAP information.

Omer et al. \cite{Arap2015AdaptiveRDForCC} propose a method for decoupling the synchronization between rounds of a recursive doubling allreduce algorithm. 
This is accomplished by removing the strict ordering imposed through rounds of communication and instead managing messages through tag values.
The relaxed ordering can possibly lead to duplicate reductions being done, so ranks are responsible for tracking which sets of reduced ranks they've received and when they need to drop messages.
They evaluated their work on a NetFPGA platform \cite{Lockwood2007NetFPGA}, allowing them to fully offload their collective algorithm, and exploit network-level features like multicast, but limited their work to only use min/max operations.

Marendic et al. \cite{Marendic2016Clairvoyant} propose a PAP-aware reduction algorithm.
Through theoretical analysis, they identify a lower bound for PAP-Aware reduction, demonstrating that no matter the PAP, any algorithm is bound by the times it takes for two processes to make a reduction, so they focus their efforts on doing that step as fast as possible. 
Their solution is a greedy algorithm to build a reduction schedule, but they have no method of detecting the PAP and assume it is known beforehand.

Proficz published a series of algorithms for different collectives, all based on a process arrival estimation method \cite{Proficz2018ImprvAllReduceForImbPAP, Proficz2020PAPAwareScatterGather, Proficz2021AllGatherResilientToImbPAP}.
Their method targets a bulk-synchronous parallel type application, where there are distinct computation and communication phases.
In order to estimate a process's arrival time, application developers embed a callback to notify a background process when the computation phase is almost complete.
This background thread uses this information to reorder processes in a collective algorithm as to make more optimal use of arrival imbalance.
The author proposes methods of reordering direct, ring, and binomial algorithms to accelerate allreduce, scatter, gather and allgather collectives.

Mohammadalizadehbakhtevari \cite{Mohammadalizadehbakhtevari2021Thesis} presented a series of ideas for handeling PAP in collectives. 
The author proposed two methods, targeting both small and large messages, for handeling arrival syncronization and message exchanges within a node.
The proposed work relies on shared memory, but they also evalute the efficacy of extending to a hierarchical algorithm to handle cluster-wide collectives.

\section{Method}
In order for ranks to determine the arrival order, we use a structure consisting of a counter for arrival position, and an array for arrival-to-rank translations.
This datastructure resides in netowrk exposed memory on a predetermined process.
When a process arrives at the collective, it fetch-and-increments the couter and then writes its rank into the arrival array indexed at its arrival position.
The accessing the counter creates a critical section, which can potentialy add overhead if not accounted for.
The memory requirements scale linearly with the number of processes, but is well within reason for the system we evaluated on.
Futhur, a more complicated design could distribute the arrival array so that each process exposes one index, more evenly distirbuting both memory requirements and network resource demand.

When a process arrives it can determine which ranks have arrived before it by indexing their arrival position in the arrival-to-rank translation array.
This setups allows us to build reduction/broadcast trees in terms of process' arrvial posittion.
One sticking point is that later arriving ranks can easily know the rank of earlier processes, but earlier processes won't know the value of later ranks untill they've arrived.
MPI ranks must know the destination before they can send their data, so if an early ranks needs to send data to a later rank it can either poll that rank's location in remote memory, or the late rank can notify the earlier process once it arrives.
While this does create a litle overhead, it is negligbale compared to large message transfer time.

In desinging an algorithm to outperform ring/rsa, our approach to is to minimize the amount of time the last arriving process needs to take, i.e. the last process should perform less work than $2n(\beta+\gamma)$. 
If we simplify the problem and look at a 2 process allreduce, the minimum amout of work involves recieving a message, performing a local reduction and sending a message.
We can remove the initial recive by sending the data ahead of time to pre-determined location, and the final send will need to become a broadcast. 
We also want to incoporate pipelining into our solution to furthur overlap the reduction message transfers.
Therefore, out proposed solution involves the last arriving process recieving the fully reduce data of the other $n-1$ ranks before it arrives, and then triggering a pipelined local reduce/broadcast to distribute the data as efficiently as possible. 
Theoreticly, the last proces only needs to reuduce and send one message divided accross $k$ segments, i.e. $T_{last\_proc\_reduce}=k\alpha+n(\beta+\gamma)$.
This is roughly half of ring and rsa bandwith requirements.

To ensure scalability, we embed our method in a hierarchical algorithm. 
Ranks perform an intranode reduction to a local leader process, then the leader processes perform the PAP-Aware inter-node allreduce, the intra-node bcast is also incorporated into the pipeline used to distribute the data at the end of the collective. 
The intranode reduction/bcast is a binomial-tree, while the internode reduction and broadcast are based on a linear-tree where arrival $r$ recieved data from $r-1$ and sends data to $r+1$.

\section{Evaluation}
\subsection{Syncronizatoin method benchmark}
In order to evaluate the difference between MPI and UCX, we wrote a benchmark to evaluate the overhead of one-sided operations and synchronization, the structure of which is provided in algorithm \ref{alg:sync_struct_bmark}.
The core of the benchmark is evaluating how long it takes to atomically increment a memory location and perform a write to a separate location.
We built two implementaions of this benchmark, one with MPI and the other with UCX. 
The most critical parts of the benchmarks would be the remote syncronization calls (lines 9 and 11), this is where the differneces between MPI and UCX will become apperent.  
In order to ensure memory completion in MPI, users need to open and close and access epoch, this is done with blocking calls to MPI\_Win\_lock / MPI\_Win\_unlock before and after each rempote memory call.
On the other hand, completion in UCX is gaurenteed at a per-operation level. 
Every communication call returns a pointer to a memory reigon which indicates the status of the operation. 
At some point in the future (during a call to ucp\_progress()), the UCX runtime will update that address to indicate completion.
So to wait for remote memory completion, the UCX benchmark loops on a call to ucp\_progress() checking completion pointer after each invocatoin.

We ran our benchmark on Narval, a 200G HDR InfiniBand cluster, using 32 nodes and scaling from 1 to 32 processes per node, and the results are presented in figure \ref{fig:sync_bmark_32n}.
As is evident, the synchronization with MPI has much more overhead than UCX.
The benchmark time of UCX scales linearly with the number of processes from 10$\mu s$ for 32 processes to 400$\mu s$ for 1024.
MPI has more overhead, as with 32 processes, it still takes 312$\mu s$, and scales exponentially to 453737$\mu s$ at 1024 processes.
Therefore, we decided to use UCX to build our PAP management mechanism, as UCX provides wrappers around the nececary RDMA and atomic network operations, there is no enforced synchronization model, and there are strong guarantees on operation completion. 

\input{3_Chapters/5_Chapter_PAPAwareness/Figs/alg_sync_struct_mbark}
\input{3_Chapters/5_Chapter_PAPAwareness/Figs/eval_sync_bmark}

\subsection{Software}
We evaluated the PAP impact using a microbenchmark similar to \cite{Faraj2008StudyProcArrivalMPIColl}, presentied in listing \ref{alg:mif_microbmark}.
The benchmark evaluates allreduce performance for different maximum imbalance factors. 
It does this by applying a random delay to each process, with process 0 recieving no delay and process $n-1$ recievein the full delay (lines 15-21).
The delay is applied through a call to usleep (line 25).
In preveous work \cite{Faraj2008StudyProcArrivalMPIColl, Alizadeh2022PAPCollDL} delays were applied thorugh a random computation, this work opted to use usleep as it provides a more percise controll.

\input{3_Chapters/5_Chapter_PAPAwareness/Figs/alg_mif_bmark}

\subsection{Hardware}
Evaluated on two clusters, Beluga and Narval.
Beluga's architecture is defined in \ref{sec:topo-eval-hardware}.
Narval's architecture is nuts.

\section{I can't belive it's not a conclusion â„¢}
High level recap

