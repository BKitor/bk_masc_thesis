% Chapter 5 - Process Arrival Pattern Awareness

\glsresetall % reset the glossary to expand acronyms again
\chapter[PAPAwareness]{Process Arrival Pattern Awareness}\label{ch:PAPAwareness}
\index{Process Arrival Pattern Awareness}

\begin{itemize}
    \item Background on Process Arrival Pattern Awareness
    \begin{itemize}
        \item Maximum/Average Imbalance Factor and EQs
        \item Allreduce with MIF Micro-Benchmark and code-snippet
        \begin{itemize}
            \item Identify syncronizatoin/data dependencies, porpose solution to syncronizaiotn dependencies using callbacks \cite{Luo2018ADAPT}
        \end{itemize}
    \end{itemize}
    \item Theoretical modeling
    \item Methods
    \begin{itemize}
        \item Syncstructue, setup on process 0, modified through RMA operations
        \item UCX used due to low syncronization overhead - MPI RMA ops require proper access epochs, which add tons of overhead
        \begin{itemize}
            \item "Concurrent accumulate operations with different origin and target pairs are
not ordered. Thus, there is no guarantee that the entire call to an accumulate operation is
executed atomically."(32-34 page 618)
        \end{itemize}
        \item Background thread to recv messages before the process has arrived 
        \item Hierarchical alg 
    \end{itemize}
    \item Evaluation
    \begin{itemize}
        \item Micro-Benchmark
            \begin{itemize}
                \item Outline method of applying pap-awareness? or just link preveous paper?
            \end{itemize}
        \item Horovod? Cosmoflow?
    \end{itemize}
\end{itemize}

\section{Motivation}

This chapter approaches allreduce collective design throught the lesne of processs arrival pattern (PAP) awareness. 
When desinging collecive alaogirhtms developers often assume that all processes arrive at the same time, but this is not a safe assumption.
The order and timing that processes enter the collective can have an impact on the algorithm's performance, and this work tries to leverage the arrival imbalance to improve the overall collective performance.

There are preveously proposed methods and algorithms that leverage PAP awareness to accelerate collectives, this chapter starts by surveying existing work.
We propose a cluster-wide method of sharing PAP information along with an allreduce algorithm based on this method that performs better than existing algorithms under a large processes arrival imbalance.
We evaluate our work with using an imbalance factor inducing benchmark, and show that we can see \_\% imrpovement over state of the art algorithms.  
(Horovod or Cosmoflow?)

\section{Background}

\subsection{One Sided Operations}

\begin{itemize}
    \item Traditional MPI has two-side scemantics, p2p operations, easy to understand and debug
    \item One-sided operatoins, processes don't syncronize, one process acceses another process' memory unbenownced to the target process
    \item High level libraries include: MPI2-RMA, Chappel, SHMEM. Provide high level interfaces tailored to building apps and hiding complexity. Hallmarks include datatypes, process management, syncronizatoin models.
    \item Under the hood, implementations target a transport focued layer to run the hardware. Libfabric or UCX. Provides balance between portability and performance. More flexible interface designed to make full use of available hardware, not ment to be used by Physisits and Chemists.
    \item MPI syncronizatoin is to tight, requiring an access epoch in order to modify shared memory.
    
\end{itemize}



\subsection{Related Work}

Mamidala et al. \cite{Mamidala2004BarrierAllreduceIBAdaptive} propose a PAP-Aware tree based algorithm that can be applied to barrier and allreduce collectives.
Their method invovles rebalanceing a k-ary tree by passing a token between ranks.
For the process holding the token, when $k-1$ child processes have arrived, the token is passed to the $k^{th}$ child thorugh an RDMA write.
If an arriving process is holding the token and all it's child processes have arrived, it knows it's the last to arrive, so it trigers a multicast releasing all ranks from the barrier. 

Faraj et al. \cite{Faraj2008StudyProcArrivalMPIColl} evaluate process arrival imbalance at collectives accross a handful of MPI kernels.
The PMPI profiling interface was used to introspect on the kernel runtime and collect PAP statistics on two different clusters.
The authors determine that process imbalance in unavoidable, even if the workload is perfectly ballanced at the applictaoin level, the complexity of these massive systems will inevitably lead to diffrerencess at arrival time. 
But, regular imbalance patterns do emerge during application runtime, specific collective callsites will exebit the same imbalance multiple times during excecution.
They also measure the affect of processes imbalance on specific algorithms for broadcast and alltoall outlining how processes imbalance can be an important factor on algorithm selection.
The authors propose a method for PAP-Aware dynamic algorithm selection, bsed on STAR-MPI \cite{Faraj2006StarMPI}.
This method monitors collective excecution at each method invocatoin, and selects an optimal algorithm based on observed PAP Imbalance. 

Patarsuk and Yuan \cite{Patarasuk2008EffBcastDifProcArr} investigate the impacts of PAP on broadcast algorithms.
Through modeling, they show how all existing algorithms can suffer from substancial performance loss to process imbalance. 
The authors propose a new broadcast algorithm that dynamicly assembles sub-groups to perform the broadcast operation.

Qian and Afsahi \cite{Qian2009ProcArrivalSHMA2AIB} propose a method for applying PAP-Awareness to alltoall in Infiniband Clusters.
They modify a direct alltoall algorithm so that data exchanges are not ordered.
The algorithm is built on top of infiniband's RDMA semantics, which means processes need to share destination addresses that peers can write data into. 
Their method relies on using the RDMA address as a notification mechanism alerting processes of when their peers have arrived, and where to write relevant data.
They also extend this idea to a hierarchical type algorithm, alowing them to leverage shared memory for intranode transfers.

Parsons and Pai \cite{Parsons2015ExpProcImbMPICollHierarcialSys} study process imbalance their Cray XE6 in a similar way to Faraj et al. \cite{Faraj2008StudyProcArrivalMPIColl}.
They go a bit more in depth by investigating performance counters using PAPI \cite{Mucci1999PAPI}, but they arrive at the same conclusion that the systme is to complex and that none of the observable counters strongly corelate to process imbalance. 
The authors propose a dynamic leader selection method to build hierarchical pap-aware algorithms for reduce, broadcast.
They use a shared memory structure for intranode communications, where the last/first processes to arive is selected as the leader for the reduce/broadcast algortihm respectivly. 
Since any rank could be dynamicly selected as a leader, parent/child relationships for the internode binomial tree are established using controll messaged with MPI\_ANY\_SOURCE. 
They also propose an alltoall algorthm, but they found that the overhead of the controll messages is to great so they impose a static multileader hierarchicla pattern.
In order to take advantage of arrival patterns, the authrors propose \textit{oppertunistic message fragmentation}, a criteria that leaders can to to select chunks of data to send before all processes have arrived.

Omer et al. \cite{Arap2015AdaptiveRDForCC} propose a method for decoupling the syncronization between rounds of a recurisve doubling allreduce algorithm. 
This is accomplished by removing the strict ordering imposed through rounds of communicatoin, and instead managing messages through tag values.
They evaluated their work on a NetFPGA platform \cite{Lockwood2007NetFPGA}, alowing them to fully offload their collective algorithm, exploit network level optimizatoins like multicast, but limited their work to only use min/max operations.

Marendic et al. \cite{Marendic2016Clairvoyant} propose a PAP-aware reduction algorithm.
Through theoreticla analysis, they identify a lower bound for PAP-Aware reduction, demonstrating that no matter the PAP, any algorithm is bound by the times it takes for 2 processs to do a reduction, so they focus their effor on doing that step as fast as possible. 
Their solution is a greedy algorithm to build reduction schedule, but they have no method of detecting the PAP and assume it is know beforehand.

Proficz published a series of algorithsm for different collectives all based on a process arrrival estimation method \cite{Proficz2018ImprvAllReduceForImbPAP, Proficz2020PAPAwareScatterGather, Proficz2021AllGatherResilientToImbPAP}.
Their method targest a bulk-syncronus parallel type application, where there are distinct computation and communication phases.
In order to estimate a processe's arrival time, application developers embed an callback to notify a background process when the computation phase is almost complete.
This background thread uses this information to reorder processes in a collective algorithm as to make more optimal use of arrival imbalance.
The author proposes methods of reordering direct, ring, and binomial algorithms to accelerate allreduce, scatter, gather and allgather collectives.

\section{Method}


\section{Results}

