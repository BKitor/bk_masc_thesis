% Chapter 5 - Process Arrival Pattern Awareness

% \glsresetall % reset the glossary to expand acronyms again
\chapter[Process Arrival Pattern Awareness]{Process Arrival Pattern Awareness}\label{ch:PAPAwareness}
\index{Process Arrival Pattern Awareness}

\begin{itemize}
    \item Background on Process Arrival Pattern Awareness
    \begin{itemize}
        \item Pedram's characterization work, mention that Horovod can see PAP imbalacen in range of 2-6, \cite{Alizadeh2022PAPCollDL, Mohammadalizadehbakhtevari2021Thesis}
        \item Maximum/Average Imbalance Factor and EQs
    \end{itemize}
    \item Methods
    \item Evaluation
    \begin{itemize}
        \item Horovod? Cos  moflow?
    \end{itemize}
\end{itemize}

\section{Motivation}

This chapter approaches allreduce collective design through the lens of process arrival pattern (PAP) awareness. 
When designing collective algorithms, developers often assume that all processes arrive at the same time, but this is not a safe assumption.
The order and timing that processes enter the collective can have an impact on the algorithm's performance, and this work tries to leverage the arrival imbalance to improve the overall collective performance.
Deep learning is a difficult application to load balance due to the stochastic nature of datasets, leading to increased arrival imbalance at collectives \cite{Mohammadalizadehbakhtevari2021Thesis, Alizadeh2022PAPCollDL, Li2020DLPartialColl}. 
While PAP-aware collective algorithms do exist, no existing algorithms target multi-node GPU deployments, so to fill this gap, we propose a UCX-RMA-based PAP distribution mechanism with an accompanying allreduce algorithm which shows \_\% improvement over default allreduce algorithms performance under arrival imbalance.  

There are previously proposed methods and algorithms that leverage PAP awareness to accelerate collectives, this chapter starts by surveying existing work.
We outline some analysis on how PAP affects collectives, outlining our approach to accelerate allreduce.
The main contribution is a novel cluster-wide method of sharing PAP information with an accompanying allreduce algorithm designed to perform better than existing algorithms under a processes arrival imbalance.
We evaluate our work using an imbalance factor-inducing benchmark and show that we can see \_\% improvement over state-of-the-art algorithms.  


\subsection{Related Work}
Mamidala et al. \cite{Mamidala2004BarrierAllreduceIBAdaptive} propose a PAP-Aware tree-based algorithm that can be applied to barrier and allreduce collectives.
Their method involves rebalancing a k-ary tree by passing a token between ranks.
For the process holding the token, when $k-1$ child processes have arrived, the token is passed to the $k^{th}$ child through an RDMA write.
If an arriving process is holding the token and all its child processes have arrived, it knows it's the last to arrive, so it triggers a multicast releasing all ranks from the barrier. 

Faraj et al. \cite{Faraj2008StudyProcArrivalMPIColl} evaluate process arrival imbalance at collectives across a handful of MPI kernels.
The PMPI profiling interface was used to introspect the kernel runtime and collect PAP statistics on two different clusters.
The authors determine that process imbalance is unavoidable; even if the workload is perfectly balanced at the application level, the complexity of these massive systems will inevitably lead to differences in arrival time. 
But, regular imbalance patterns do emerge during application runtime, and specific collective call sites will exhibit the same imbalance multiple times during execution.
They also measure the effect of process imbalance on specific algorithms for broadcast and alltoall outlining how process imbalance can be an important factor in algorithm selection.
The authors propose a method for PAP-Aware dynamic algorithm selection based on STAR-MPI \cite{Faraj2006StarMPI}.
This method monitors collective execution at the granularity of each call site, and selects an optimal algorithm based on observed PAP Imbalance. 

Patarsuk and Yuan \cite{Patarasuk2008EffBcastDifProcArr} investigate the impacts of PAP on broadcast algorithms.
Through modelling, they show how all existing algorithms can suffer from substantial performance loss to process imbalance. 
The authors propose a new broadcast algorithm that dynamically assembles sub-groups to perform the broadcast operation.

Qian and Afsahi \cite{Qian2009ProcArrivalSHMA2AIB} propose a method for applying PAP-Awareness to alltoall in Infiniband Clusters.
They modify a direct alltoall algorithm so that data exchanges are not ordered.
The algorithm is built on top of InfiniBand's RDMA semantics, which means processes need to share destination addresses that peers can write data into. 
Their method relies on using the RDMA address as a notification mechanism alerting processes of when their peers have arrived and where to write relevant data.
They also extend this idea to a hierarchical type algorithm, allowing them to leverage shared memory for intranode transfers.

Parsons and Pai \cite{Parsons2015ExpProcImbMPICollHierarcialSys} study process imbalance on their Cray XE6 in a similar way to Faraj et al. \cite{Faraj2008StudyProcArrivalMPIColl}.
They go a bit more in-depth by investigating performance counters using PAPI \cite{Mucci1999PAPI}, but they arrive at the same conclusion that the system is too complex and that none of the observable counters strongly correlate with processing imbalance. 
The authors propose a dynamic leader selection method to build hierarchical pap-aware algorithms for reduce and broadcast.
They use a shared memory structure for intranode communications, where the last/first processes to arrive is selected as the leader for the reduce/broadcast algorithms, respectively. 
Since any rank could be dynamically selected as a leader, parent/child relationships for the internode binomial tree are established using control messaged with \texttt{MPI\_ANY\_SOURCE}. 
They also propose an alltoall algorithm, but they found that the overhead of the control messages is too great, so instead they impose a static multileader hierarchical pattern.
In order to take advantage of arrival patterns, the authors propose \textit{opportunistic message fragmentation}, criteria that leaders can use to select chunks of data to send before all processes have arrived.
This work only applies PAP awareness at an intra-node level; while there is a leader identification mechanism to set up inter-node exchanges, it does not leverage any PAP information.

Omer et al. \cite{Arap2015AdaptiveRDForCC} propose a method for decoupling the synchronization between rounds of a recursive doubling allreduce algorithm. 
This is accomplished by removing the strict ordering imposed through rounds of communication and instead managing messages through tag values.
The relaxed ordering can possibly lead to duplicate reductions being done, so ranks are responsible for tracking which sets of reduced ranks they've received and when they need to drop messages.
They evaluated their work on a NetFPGA platform \cite{Lockwood2007NetFPGA}, allowing them to fully offload their collective algorithm, and exploit network-level features like multicast, but limited their work to only use min/max operations.

Marendic et al. \cite{Marendic2016Clairvoyant} propose a PAP-aware reduction algorithm.
Through theoretical analysis, they identify a lower bound for PAP-Aware reduction, demonstrating that no matter the PAP, any algorithm is bound by the times it takes for two processes to make a reduction, so they focus their efforts on doing that step as fast as possible. 
Their solution is a greedy algorithm to build a reduction schedule, but they have no method of detecting the PAP and assume it is known beforehand.

Proficz published a series of algorithms for different collectives, all based on a process arrival estimation method \cite{Proficz2018ImprvAllReduceForImbPAP, Proficz2020PAPAwareScatterGather, Proficz2021AllGatherResilientToImbPAP}.
Their method targets a bulk-synchronous parallel type application, where there are distinct computation and communication phases.
In order to estimate a process's arrival time, application developers embed a callback to notify a background process when the computation phase is almost complete.
This background thread uses this information to reorder processes in a collective algorithm as to make more optimal use of arrival imbalance.
The author proposes methods of reordering direct, ring, and binomial algorithms to accelerate allreduce, scatter, gather and allgather collectives.

Mohammadalizadehbakhtevari \cite{Mohammadalizadehbakhtevari2021Thesis} presented a series of ideas for handeling PAP in collectives. 
The author proposed two methods, targeting both small and large messages, for handeling arrival syncronization and message exchanges within a node.
The proposed work relies on shared memory, but they also evalute the efficacy of extending to a hierarchical algorithm to handle cluster-wide collectives.

\section{Method}
In order for ranks to determine the arrival order, we use a structure consisting of a counter for arrival position, and an array for arrival-to-rank translations.
This datastructure resides in netowrk exposed memory on a predetermined process.
When a process arrives at the collective, it fetch-and-increments the couter and then writes its rank into the arrival array indexed at its arrival position.
The accessing of the counter creates a critical section, which can potentialy add overhead if not accounted for.
The memory requirements scale linearly with the number of processes, but is well within reason for the system we evaluated on.
Futhur, a more complicated design could distribute the arrival array so that each process exposes one index, more evenly distirbuting both memory requirements and network resource demand.

When a process arrives it can determine which ranks have arrived before it by indexing their arrival position in the arrival-to-rank translation array.
This setups allows us to build reduction/broadcast trees in terms of process' arrvial posittion.
One sticking point is that later arriving ranks can easily know the rank of earlier processes, but earlier processes won't know the value of later ranks untill they've arrived.
MPI ranks must know the destination before they can send their data, so if an early ranks needs to send data to a later rank it can either poll that rank's location in remote memory, or the late rank can notify the earlier process once it arrives.
While this does create a litle overhead, it is negligbale compared to large message transfer time.

In terms of implementation we want to leverage a higher-level programming interface for portability, and our first choice would be MPI's one-sided communications, but the problem with MPI is that message completion and synchronization model is too burdensome. 
The counter can be incremented though \texttt{MPI\_Fetch\_and\_op()}, but atomicity and completion has to be enforced thorugh a syncronization mechanism. 
Active target communication is out of the question as one of our goals is to minimize the amount of synchronization, and \texttt{MPI\_Win\_Fence()} is a collective, so it also defeats the purpose.
This leaves \texttt{MPI\_Win\_lock()}/\texttt{MPI\_Win\_unlock()} as the only viable synchronization mechanism.
Alternatively, we could go a bit deeper down the stack and use a transport layer API since they provide the desired portability with an aceptable performance penalty.
Specificly, we use UCX's RMA and Atomic operations, \texttt{ucp\_atomic\_op\_nbx()} is specified to be atomic accross all other network operations alowing us to atomicly increment the counter variable, and operation completion is gaurneteed on a per-message level, removing the requirements for the heavy barrier-like syncronizations MPI enforces.

In desinging an algorithm to outperform ring/rsa, our approach to is to minimize the amount of time the last arriving process needs to take, i.e. the last process should perform less work than $2n(\beta+\gamma)$. 
If we simplify the problem and look at a 2 process allreduce, the minimum amout of work involves recieving a message, performing a local reduction and sending a message.
We can remove the initial recive by sending the data ahead of time to pre-determined location, and the final send will need to become a broadcast. 
We also want to incoporate pipelining into our solution to furthur overlap the reduction message transfers.
Therefore, out proposed solution involves the last arriving process recieving the fully reduce data of the other $n-1$ ranks before it arrives, and then triggering a pipelined local reduce/broadcast to distribute the data as efficiently as possible. 
Theoreticly, the last proces only needs to reuduce and send one message divided accross $k$ segments, i.e. $T_{last\_proc\_reduce}=k\alpha+n(\beta+\gamma)$.
This is roughly half of ring and rsa bandwith requirements.

To ensure scalability, we embed our method in a hierarchical algorithm. 
Ranks perform an intranode reduction to a local leader process, then the leader processes perform the PAP-Aware inter-node allreduce, the intra-node bcast is also incorporated into the pipeline used to distribute the data at the end of the collective. 
The intranode reduction/bcast is a binomial-tree, while the internode reduction and broadcast are based on a linear-tree where arrival $r$ recieved data from $r-1$ and sends data to $r+1$.

\section{Evaluation}
\subsection{Syncronizatoin method benchmark}
In deciding to use UCX over MPI, we wrote a benchmark to evaluate the overhead of one-sided operations and synchronization, the structure of which is provided in algorithm \ref{alg:sync_struct_bmark}.
The core of the benchmark is evaluating how long it takes to atomically increment a memory location and perform a write to a separate location.
We built two implementaions of this benchmark, one with MPI and the other with UCX. 
The most critical parts of the benchmarks would be the remote syncronization calls (lines 9 and 11), this is where the differneces between MPI and UCX will become apperent.  
In order to ensure memory completion in MPI, users need to open and close and access epoch, this is done with blocking calls to \texttt{MPI\_Win\_lock()}/\texttt{MPI\_Win\_unlock()} before and after each rempote memory call.
On the other hand, completion in UCX is gaurenteed at a per-operation level. 
Every communication call returns a pointer to a memory reigon which indicates the status of the operation, and at some point in the future (during a call to \texttt{ucp\_progress()}) the UCX runtime will update that address to indicate completion.
So to wait for remote memory completion, the UCX benchmark polls the pointer while calling \texttt{ucp\_progress()} until completion.

We ran our benchmark on Narval, a 200G HDR InfiniBand cluster, using 32 nodes and scaling from 1 to 32 processes per node, and the results are presented in figure \ref{fig:sync_bmark_32n}.
As is evident, the synchronization with MPI has much more overhead than UCX.
The benchmark time of UCX scales linearly with the number of processes from 10$\mu s$ for 32 processes to 400$\mu s$ for 1024.
MPI has more overhead, as with 32 processes, it still takes 312$\mu s$, and scales exponentially to 453737$\mu s$ at 1024 processes.
Therefore, we decided to use UCX to build our PAP management mechanism, as UCX provides wrappers around the nececary RDMA and atomic network operations, there is no enforced synchronization model, and there are strong guarantees on operation completion. 

\input{3_Chapters/5_Chapter_PAPAwareness/Figs/alg_sync_struct_mbark}
\input{3_Chapters/5_Chapter_PAPAwareness/Figs/eval_sync_bmark}

\subsection{Software}
We evaluated the PAP impact using a microbenchmark similar to \cite{Faraj2008StudyProcArrivalMPIColl}, presentied in listing \ref{alg:mif_microbmark}.
The benchmark evaluates allreduce performance for different maximum imbalance factors. 
It does this by applying a random delay to each process, with process 0 recieving no delay and process $n-1$ recievein the full delay (lines 15-21).
The delay is applied through a call to usleep (line 25).
In preveous work \cite{Faraj2008StudyProcArrivalMPIColl, Alizadeh2022PAPCollDL} delays were applied thorugh a random computation, this work opted to use usleep as it provides a more percise controll.

\input{3_Chapters/5_Chapter_PAPAwareness/Figs/alg_mif_bmark}

\subsection{Hardware}
Evaluated on two clusters, Beluga and Narval.
Beluga's architecture is defined in \ref{sec:topo-eval-hardware}.
Narval is very similar to Beluga, but is a newer generation system.
Navval is a fat-tree cluster built on HDR InfiniBand, which can support 200Gb/s speeds, but instead Narval uses splitter cabels to create super fat leaf switches at 100Gb/s speeds, this gives it a blocking factor of 4.7:1.
The GPU nodes have 4 Nvidia A100 GPUs which are fully connected by NVLink and hosted by two MAD Epyc 7413 (codename Milan) for a total of 48 cores per node.

\input{3_Chapters/5_Chapter_PAPAwareness/Figs/omb_beluga}

\input{3_Chapters/5_Chapter_PAPAwareness/Figs/omb_narval}

% \input{some sort of table for Horovod Performance}

\section{I can't belive it's not a conclusion â„¢}
High level recap

